{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('tm_navigator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import codecs\n",
    "import h5py\n",
    "import itertools as it\n",
    "\n",
    "import sqlalchemy as sa\n",
    "import sqlalchemy.ext.hybrid\n",
    "import sqlalchemy.ext.declarative as sa_dec\n",
    "import sqlalchemy_searchable as searchable\n",
    "from sqlalchemy_sqlschema import maintain_schema\n",
    "\n",
    "from tm_navigator.models import *"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import logging\n",
    "\n",
    "logging.getLogger('sqlalchemy.engine').setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "engine = sa.create_engine('postgresql+psycopg2://@localhost/tm_navigator_dev', echo=0)\n",
    "sa.orm.configure_mappers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Session = sa.orm.sessionmaker(bind=engine)\n",
    "\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def session_scope():\n",
    "    \"\"\"Provide a transactional scope around a series of operations.\"\"\"\n",
    "    session = Session()\n",
    "    try:\n",
    "        yield session\n",
    "        session.commit()\n",
    "    except:\n",
    "        session.rollback()\n",
    "        raise\n",
    "    finally:\n",
    "        session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Base.metadata.create_all(engine,\n",
    "                         tables=map(lambda c: c.__table__, models_public))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetMeta(id=1, schema_name='tmnav_dataset_1', title=None)>\n"
     ]
    }
   ],
   "source": [
    "with session_scope() as session:\n",
    "    ds = DatasetMeta()\n",
    "    session.add(ds)\n",
    "    ds = session.query(DatasetMeta).order_by(DatasetMeta.id.desc()).first()\n",
    "    print(ds)\n",
    "    \n",
    "    ds.activate_schemas()\n",
    "    Base.metadata.create_all(engine,\n",
    "                             tables=map(lambda c: c.__table__, models_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TopicModelMeta(dataset_id=1, id=1, schema_name='tmnav_topicmodel_1', title=None)>\n"
     ]
    }
   ],
   "source": [
    "with session_scope() as session:\n",
    "    ds = session.query(DatasetMeta).order_by(DatasetMeta.id.desc()).first()\n",
    "    session.add(TopicModelMeta(dataset=ds))\n",
    "    tm = session.query(TopicModelMeta).order_by(TopicModelMeta.id.desc()).first()\n",
    "    print(tm)\n",
    "    \n",
    "    tm.activate_schemas()\n",
    "    Base.metadata.create_all(engine,\n",
    "                             tables=map(lambda c: c.__table__, models_topic + models_assessment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with session_scope() as session:\n",
    "    session.query(TopicModelMeta).delete()\n",
    "    session.query(DatasetMeta).delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with session_scope() as session:\n",
    "    schemas_matching = session.query('schema_name from information_schema.schemata') \\\n",
    "        .filter(sa.column('schema_name').startswith('tmnav_')) \\\n",
    "        .subquery()\n",
    "    schemas_used = session.query(DatasetMeta.schema_name) \\\n",
    "        .union(session.query(TopicModelMeta.schema_name))\n",
    "#     schemas_unused = session.query('schema_name').select_from(schemas_matching) \\\n",
    "#         .filter(sa.column('schema_name').notin_(schemas_used))\n",
    "    schemas_unused = session.query('schema_name').select_from(schemas_matching)\n",
    "    for schema, in schemas_unused:\n",
    "        session.execute(sa.schema.DropSchema(name=schema, cascade=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with session_scope() as session:\n",
    "    SchemaMixin.activate_public_schema(session)\n",
    "    Base.metadata.drop_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy_utils import aggregates\n",
    "\n",
    "aggregates.local_condition = lambda prop, objects: sa.literal(True)\n",
    "\n",
    "class ListSession(list):\n",
    "    def execute(self, query):\n",
    "        print(query)\n",
    "        return session.execute(query)\n",
    "\n",
    "def update_aggregates(*classes_modified):\n",
    "    aggregates.manager.construct_aggregate_queries(\n",
    "        ListSession([c() for c in classes_modified]),\n",
    "        None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from io import StringIO\n",
    "from contextlib import closing\n",
    "\n",
    "def to_csv(rows, csv_f):\n",
    "    firstrow = next(rows)\n",
    "    fieldnames = firstrow.keys()\n",
    "\n",
    "    writer = csv.DictWriter(csv_f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerow(firstrow)\n",
    "    writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def copy_to_table(session, model, rows):\n",
    "    firstrow = next(rows)\n",
    "    fieldnames = firstrow.keys()\n",
    "\n",
    "    with closing(StringIO()) as csv_f:\n",
    "        writer = csv.DictWriter(csv_f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerow(firstrow)\n",
    "        writer.writerows(rows)\n",
    "        csv_f.seek(0)\n",
    "\n",
    "        with session.connection().connection.cursor() as cursor:\n",
    "            cursor.copy_expert('copy %s (%s) from stdin with csv header' % (model.__tablename__, ', '.join(fieldnames)), csv_f)\n",
    "            \n",
    "    \n",
    "    update_aggregates(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with session_scope() as session:\n",
    "    ds = session.query(DatasetMeta).filter_by(id=1).one()    \n",
    "    ds.activate_schemas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/modalities.csv', 'w') as csv_f:\n",
    "    to_csv(({'id': i+1, 'name': n} for i, n in enumerate(['words', 'authors'])), csv_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File('data/data.hdf') as h5f:\n",
    "    metadata = h5f['metadata'][...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/dictionary.mmro.txt') as f, open('data/terms.csv', 'w') as csv_f:\n",
    "    rows = (dict(id=i, modality_id=1, text=line.strip())\n",
    "            for i, line in enumerate(f))\n",
    "    \n",
    "    doc_authors = [(i, author.strip())\n",
    "         for i, m in enumerate(metadata)\n",
    "         for author in m['authors'].split(',')\n",
    "    ]\n",
    "    authors = {a for d, a in doc_authors}\n",
    "    authors_terms = {a: dict(id=i, modality_id=2, text=a)\n",
    "                     for i, a in enumerate(authors)}\n",
    "    rows = it.chain(rows, authors_terms.values())\n",
    "    \n",
    "    to_csv(rows, csv_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/documents.csv', 'w') as csv_f:\n",
    "    rows = (\n",
    "        dict(id=i, title=t['title'], file_name=t['filename'], slug=t['slug'],\n",
    "             source=re.sub(r'^\\d{4}-([A-Z]+)(\\d+)/.+', r'\\1-\\2', t['filename']),\n",
    "             html=open('data/html_sprites/%s.html' % t['filename']).read())\n",
    "        for i, t in enumerate(metadata)\n",
    "    )\n",
    "    to_csv(rows, csv_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data/document_terms.csv', 'w') as csv_f, open('data/documents.mmro.txt') as f:\n",
    "    rows = (dict(document_id=d, modality_id=1, term_id=w, count=cnt)\n",
    "             for d, line in enumerate(f)\n",
    "             for w, cnt in Counter(int(dw.split()[0]) for dw in line.split(';')[:-1]).items())\n",
    "\n",
    "    rows = it.chain(rows,\n",
    "                    (dict(document_id=d, modality_id=2, term_id=authors_terms[a]['id'], count=1)\n",
    "                     for d, a in doc_authors))\n",
    "    to_csv(rows, csv_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phi = sp.coo_matrix(np.load('data/phi.npy'))\n",
    "theta = sp.coo_matrix(np.load('data/theta.npy'))\n",
    "\n",
    "pwt = phi.A\n",
    "ptd = theta.A\n",
    "\n",
    "pd = 1.0 / theta.shape[1]\n",
    "pt = (ptd * pd).sum(1)\n",
    "pw = (pwt * pt).sum(1)\n",
    "ptw = pwt * pt / pw[:, np.newaxis]\n",
    "pdt = ptd * pd / pt[:, np.newaxis]\n",
    "\n",
    "t_to_id = lambda level, t: level * 1000 + t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/topics.csv', 'w') as csv_f:    \n",
    "    rows = (dict(id=t_to_id(1, t), type='foreground' if t < 50 else 'background', probability=p)\n",
    "            for t, p in enumerate(pt))\n",
    "    rows = it.chain(rows, (dict(id=0, type='foreground', probability=1),))\n",
    "    to_csv(rows, csv_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/topic_terms.csv', 'w') as csv_f:\n",
    "    rows = (dict(topic_id=t_to_id(1, t), modality_id=1, term_id=w, prob_wt=val, prob_tw=ptw[w, t])\n",
    "            for w, t, val in zip(phi.row, phi.col, phi.data))\n",
    "    to_csv(rows, csv_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/document_topics.csv', 'w') as csv_f:\n",
    "    rows = (dict(topic_id=t_to_id(1, t), document_id=d, prob_td=val, prob_dt = pdt[t, d])\n",
    "            for t, d, val in zip(theta.row, theta.col, theta.data))\n",
    "    to_csv(rows, csv_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/topic_edges.csv', 'w') as csv_f:\n",
    "    rows = (dict(parent_id=0, child_id=t_to_id(1, t), probability=p)\n",
    "            for t, p in enumerate(pt))\n",
    "    to_csv(rows, csv_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with session_scope() as session:\n",
    "    session.add(Modality(name='words'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATE modalities SET count=(SELECT count(*) AS count_1 \n",
      "FROM terms \n",
      "WHERE modalities.id = terms.modality_id) WHERE :param_1\n",
      "7805\n"
     ]
    }
   ],
   "source": [
    "with open('data/dictionary.mmro.txt') as f, session_scope() as session:\n",
    "    modality = session.query(Modality).filter(Modality.name == 'words').one()\n",
    "    rows = (dict(id=i, modality_id=modality.id, text=line.strip())\n",
    "            for i, line in enumerate(f))\n",
    "    copy_to_table(session, Term, rows)\n",
    "    \n",
    "    print(session.query(Term).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File('data/data.hdf') as h5f:\n",
    "    metadata = h5f['metadata'][...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1061\n",
      "CPU times: user 1.14 s, sys: 80 ms, total: 1.22 s\n",
      "Wall time: 2.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with session_scope() as session:\n",
    "    rows = (\n",
    "        dict(id=i, title=t['title'], file_name=t['filename'], slug=t['slug'],\n",
    "             source=re.sub(r'^\\d{4}-([A-Z]+)(\\d+)/.+', r'\\1-\\2', t['filename']),\n",
    "             html=open('data/html_sprites/%s.html' % t['filename']).read())\n",
    "        for i, t in enumerate(metadata)\n",
    "    )\n",
    "    copy_to_table(session, Document, rows)\n",
    "    \n",
    "    print(session.query(Document).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with session_scope() as session:\n",
    "    session.add(Modality(name='authors'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.76 s, sys: 104 ms, total: 1.86 s\n",
      "Wall time: 3.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with session_scope() as session:\n",
    "    m = session.query(Modality).filter(Modality.name == 'authors').one()\n",
    "    \n",
    "    doc_authors = [(i, author.strip())\n",
    "         for i, m in enumerate(metadata)\n",
    "         for author in m['authors'].split(',')\n",
    "    ]\n",
    "    authors = {a for d, a in doc_authors}\n",
    "    authors_terms = {a: Term(modality=m, text=a)\n",
    "                     for a in authors}\n",
    "    session.add_all(\n",
    "        DocumentTerm(document_id=d, term=authors_terms[a], count=1)\n",
    "        for d, a in doc_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATE terms SET count=(SELECT coalesce(sum(document_terms.count), :param_1) AS coalesce_1 \n",
      "FROM document_terms \n",
      "WHERE terms.modality_id = document_terms.modality_id AND terms.id = document_terms.term_id) WHERE :param_2\n",
      "[(1, 314081), (2, 2315)]\n",
      "CPU times: user 2.82 s, sys: 44 ms, total: 2.87 s\n",
      "Wall time: 18.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open('data/documents.mmro.txt') as f, session_scope() as session:\n",
    "    m_id = session.query(Modality.id).filter(Modality.name == 'words').scalar()\n",
    "    \n",
    "    rows = (dict(document_id=d, modality_id=m_id, term_id=w, count=cnt)\n",
    "             for d, line in enumerate(f)\n",
    "             for w, cnt in Counter(int(dw.split()[0]) for dw in line.split(';')[:-1]).items())\n",
    "\n",
    "    copy_to_table(session, DocumentTerm, rows)\n",
    "    \n",
    "    print(session.query(DocumentTerm.modality_id, sa.func.count()).group_by(DocumentTerm.modality_id).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.29 s, sys: 164 ms, total: 9.46 s\n",
      "Wall time: 46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open('data/documents.mmro.txt') as f, session_scope() as session:\n",
    "    m_id = session.query(Modality.id).filter(Modality.name == 'words').scalar()\n",
    "\n",
    "    id_cnt = it.count()\n",
    "    rows = (dict(id=next(id_cnt), document_id=d, modality_id=m_id, term_id=w, start_pos=s, end_pos=e)\n",
    "            for d, line in enumerate(f)\n",
    "            for w, s, e in (map(int, dw.split()) for dw in line.split(';')[:-1]))\n",
    "\n",
    "    copy_to_table(session, DocumentContent, rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phi = sp.coo_matrix(np.load('data/phi.npy'))\n",
    "theta = sp.coo_matrix(np.load('data/theta.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pwt = phi.A\n",
    "ptd = theta.A\n",
    "\n",
    "pd = 1.0 / theta.shape[1]\n",
    "pt = (ptd * pd).sum(1)\n",
    "pw = (pwt * pt).sum(1)\n",
    "ptw = pwt * pt / pw[:, np.newaxis]\n",
    "pdt = ptd * pd / pt[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with session_scope() as session:\n",
    "    ds = session.query(TopicModelMeta).filter_by(id=1).one()    \n",
    "    ds.activate_schemas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATE topics SET probability=(SELECT coalesce(sum(document_topics.prob_td), :param_1) AS coalesce_1 \n",
      "FROM document_topics \n",
      "WHERE topics.id = document_topics.topic_id) WHERE :param_2\n",
      "[(0, 1), (1, 60)]\n",
      "[(1, 14161)]\n",
      "[(1, 120936)]\n",
      "CPU times: user 3.64 s, sys: 8 ms, total: 3.65 s\n",
      "Wall time: 9.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with session_scope() as session:\n",
    "    m_id = session.query(Modality.id).filter(Modality.name == 'words').scalar()\n",
    "    \n",
    "    session.add(Topic(id=0, type='foreground'))\n",
    "\n",
    "    t_to_id = lambda level, t: level * 1000 + t\n",
    "\n",
    "    copy_to_table(session, Topic,\n",
    "                  (dict(id=t_to_id(1, t), type='foreground' if t < 50 else 'background')\n",
    "                   for t in range(phi.shape[1])\n",
    "                  ))\n",
    "    rows = (dict(topic_id=t_to_id(1, t), modality_id=m_id, term_id=w, prob_wt=val, prob_tw=ptw[w, t])\n",
    "            for w, t, val in zip(phi.row, phi.col, phi.data))\n",
    "    copy_to_table(session, TopicTerm, rows)\n",
    "\n",
    "    rows = (dict(topic_id=t_to_id(1, t), document_id=d, prob_td=val, prob_dt = pdt[t, d])\n",
    "            for t, d, val in zip(theta.row, theta.col, theta.data))\n",
    "    copy_to_table(session, DocumentTopic, rows)\n",
    "            \n",
    "    \n",
    "    print(session.query(Topic.level, sa.func.count()).group_by(Topic.level).all())\n",
    "    print(session.query(Topic.level, sa.func.count()).join(DocumentTopic).group_by(Topic.level).all())\n",
    "    print(session.query(Topic.level, sa.func.count()).join(TopicTerm).group_by(Topic.level).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with session_scope() as session:\n",
    "    root = session.query(Topic).filter(Topic.level == 0).one()\n",
    "    q = session.query(Topic).filter(Topic.level == 1)\n",
    "    session.add_all(TopicEdge(parent=root, child=t, probability=t.probability) for t in q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.8 s, sys: 56 ms, total: 4.86 s\n",
      "Wall time: 29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open('data/ptdw.txt') as fp, session_scope() as session:\n",
    "    id_cnt = it.count()\n",
    "    rows = (dict(document_content_id=next(id_cnt), topic_id=1000 + int(t))\n",
    "            for d, linep in enumerate(fp)\n",
    "            for t in linep.split())\n",
    "\n",
    "    copy_to_table(session, DocumentContentTopic, rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 556 ms, sys: 0 ns, total: 556 ms\n",
      "Wall time: 1.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with session_scope() as session:\n",
    "    distances = squareform(pdist(theta.A.T, 'cosine'))\n",
    "\n",
    "    rows = (dict(a_id=i, b_id=sim_i, similarity=1 - row[sim_i])\n",
    "            for i, row in enumerate(distances)\n",
    "            for sim_i in row.argsort()[:31]\n",
    "            if sim_i != i)\n",
    "    copy_to_table(session, DocumentSimilarity, rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 72 ms, sys: 0 ns, total: 72 ms\n",
      "Wall time: 175 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with session_scope() as session:\n",
    "    distances = squareform(pdist(phi.A.T, 'cosine'))\n",
    "\n",
    "    rows = (dict(a_id=t_to_id(1, i), b_id=t_to_id(1, sim_i), similarity=1 - row[sim_i])\n",
    "            for i, row in enumerate(distances)\n",
    "            for sim_i in row.argsort()[:]\n",
    "            if sim_i != i)\n",
    "    copy_to_table(session, TopicSimilarity, rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.76 s, sys: 224 ms, total: 8.98 s\n",
      "Wall time: 13.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with session_scope() as session:\n",
    "    m_id = session.query(Modality.id).filter(Modality.name == 'words').scalar()\n",
    "    distances = squareform(pdist(phi.A, 'cosine'))\n",
    "\n",
    "    rows = (dict(a_modality_id=m_id, b_modality_id=m_id, a_id=i, b_id=sim_i, similarity=1 - row[sim_i])\n",
    "            for i, row in enumerate(distances)\n",
    "            for sim_i in row.argsort()[:20]\n",
    "            if sim_i != i)\n",
    "    copy_to_table(session, TermSimilarity, rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.0), (1, 1.0000000060127)]\n"
     ]
    }
   ],
   "source": [
    "with session_scope() as session:\n",
    "    session.execute('''\n",
    "    with probs as (\n",
    "        select\n",
    "            topic_id,\n",
    "            sum(prob_td / (select count(*) from documents)) as probability\n",
    "        from document_topics\n",
    "        group by topic_id\n",
    "    )\n",
    "    \n",
    "    update topics\n",
    "    set probability = probs.probability\n",
    "    from probs\n",
    "    where topics.id = probs.topic_id\n",
    "    ''')\n",
    "    print(session.query(Topic.level, sa.func.sum(Topic.probability)).group_by(Topic.level).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'-'*10000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

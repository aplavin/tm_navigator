{
 "metadata": {
  "name": "",
  "signature": "sha256:e0960ab1caa3a02f9a056e732e6950acc136aa0e00019471b7021dfee8c846e4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import codecs\n",
      "import numpy as np\n",
      "import pandas\n",
      "import scipy.sparse\n",
      "from scipy.ndimage.filters import convolve1d\n",
      "import sys\n",
      "import comp_exp\n",
      "import py_artm\n",
      "import h5py\n",
      "sys.path.append('/home/alexander/strijov/MLAlgorithms/Group174/Plavin2014TopicsNumberOptimization/code')\n",
      "from utils import *\n",
      "from ggplot_utils import *\n",
      "from IPython.display import display, display_html\n",
      "from functools import partial\n",
      "from itertools import count, groupby\n",
      "from ipy_progressbar import ProgressBar\n",
      "from itertools import starmap\n",
      "import codecs\n",
      "from pprint import pprint\n",
      "from collections import Counter, defaultdict\n",
      "from recordtype import recordtype\n",
      "import re\n",
      "from glob import glob\n",
      "from HTMLParser import HTMLParser\n",
      "from htmlentitydefs import name2codepoint\n",
      "import pymorphy2\n",
      "\n",
      "from rpy2.robjects import pandas2ri\n",
      "pandas2ri.activate()\n",
      "import rpy2.robjects as robj\n",
      "from rpy2.robjects.lib import ggplot2 as gg\n",
      "from rpy2.ipython.ggplot import ggplot as iggplot\n",
      "from rpy2.robjects.packages import importr\n",
      "rbase = importr('base')\n",
      "\n",
      "%matplotlib inline\n",
      "%load_ext rpy2.ipython.rmagic\n",
      "\n",
      "from ggplot_utils import *\n",
      "MyGGPlot.latex_enabled = False"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/lib/python2.7/distutils/dist.py:267: UserWarning: Unknown distribution option: 'library_dirs'\n",
        "  warnings.warn(msg)\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "\u0421\u0447\u0438\u0442\u044b\u0432\u0430\u043d\u0438\u0435 \u041d\u0430\u0434\u0438\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ds = []\n",
      "ws = []\n",
      "cnts = []\n",
      "\n",
      "with open('MMRO_new.txt') as nwdf:\n",
      "    D = int(nwdf.readline())\n",
      "    W = int(nwdf.readline())\n",
      "    \n",
      "    for line in nwdf:\n",
      "        d, w, cnt = map(int, line.split())\n",
      "        ds.append(d)\n",
      "        ws.append(w)\n",
      "        cnts.append(cnt)\n",
      "        \n",
      "nwd = scipy.sparse.coo_matrix((cnts, (ws, ds))).astype(np.float32)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nwd.shape, sum(nwd.sum(1).A1 == 0), sum(nwd.sum(0).A1 == 0), 13031-668"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "((12363, 1009), 0, 0, 12363)"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with codecs.open('authors.txt', encoding='utf-8') as authf:\n",
      "    lines = [line.strip() for line in authf]\n",
      "    docinfos = []\n",
      "    i = 0\n",
      "    while i < len(lines):\n",
      "        assert int(lines[i]) * 3 == i\n",
      "        assert int(lines[i]) == len(docinfos)\n",
      "        i += 1\n",
      "        docinfo = []\n",
      "        while i < len(lines) and not lines[i].isdigit():\n",
      "            docinfo.append(lines[i])\n",
      "            i += 1\n",
      "        assert len(docinfo) == 2\n",
      "        docinfos.append(tuple(docinfo))\n",
      "    docinfos = np.array(docinfos)\n",
      "    docinfos = np.array(docinfos, dtype=h5py.special_dtype(vlen=unicode))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with codecs.open('dictionary_new.txt', encoding='utf-8') as dictf:\n",
      "    assert nwd.shape[0] == int(dictf.readline().strip())\n",
      "    indwords = [line.strip().split(' ', 1) for line in dictf]\n",
      "    indwords[723] = ['723', '']\n",
      "    assert all(int(ind) == i for i, (ind, w) in enumerate(indwords))\n",
      "    words = [w.lower() for ind, w in indwords]\n",
      "    words = np.array(words, dtype=h5py.special_dtype(vlen=unicode))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "words_dict = {word: w for w, word in enumerate(words)}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with codecs.open('MMROwithOrder.txt', encoding='utf-8') as docsf:\n",
      "    lines = [line.strip() for line in docsf]\n",
      "    docs = []\n",
      "    i = 0\n",
      "    while i < len(lines):\n",
      "        assert int(lines[i]) == len(docs)\n",
      "        i += 1\n",
      "        d = len(docs)\n",
      "        \n",
      "        pwts = pwt * ptd[:, d].T\n",
      "        pwts /= pwts.sum(1)[:,np.newaxis]\n",
      "        pwts[np.isnan(pwts.sum(1)), :] = 0\n",
      "        \n",
      "        topics_glob = ptd[:,d].argsort()[::-1][:np.count_nonzero(ptd[:,d])]\n",
      "        \n",
      "        doc = []\n",
      "        while i < len(lines) and not lines[i].isdigit():\n",
      "            try:\n",
      "                word, word_norm = lines[i].lower().split(' ', 1)\n",
      "            except ValueError:\n",
      "                word = lines[i].lower().strip()\n",
      "                word_norm = word\n",
      "\n",
      "            # dictionary lookup\n",
      "            try:\n",
      "                w = words_dict[word_norm]\n",
      "            except KeyError:\n",
      "                w = -1\n",
      "                \n",
      "            # topics info\n",
      "            pts = pwts[w]\n",
      "            topics = pts.argsort()[::-1][:5]\n",
      "            pts_glob = pts[topics_glob]\n",
      "            pts = pts[topics]\n",
      "                \n",
      "            doc.append((word, w, topics, pts, pts_glob))\n",
      "            i += 1\n",
      "\n",
      "        docs.append((topics_glob,\n",
      "                     np.array(map(tuple, doc),\n",
      "                             dtype=[('word', h5py.special_dtype(vlen=unicode)),\n",
      "                                    ('w', np.int32),\n",
      "                                    ('topics', np.int32, 5),\n",
      "                                    ('pts', np.float32, 5),\n",
      "                                    ('pts_glob', np.float32, len(topics_glob))])))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('filenames_new.txt') as fnamesf:\n",
      "    docfnames = [line.strip().split(' ', 1) for line in fnamesf]\n",
      "    docfnames = [(int(d), fname) for d, fname in docfnames]\n",
      "    filenames = np.zeros((max(d for d, fname in docfnames) + 1,), dtype=h5py.special_dtype(vlen=str))\n",
      "    for d, fname in docfnames:\n",
      "        filenames[d] = fname"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "\u0417\u0430\u043f\u0443\u0441\u043a \u043c\u043e\u0434\u0435\u043b\u0438"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tau = 0.2\n",
      "t_init = 50\n",
      "\n",
      "em = py_artm.plsa.PlsaEmRational(nwd,\n",
      "                                 t_init,\n",
      "                                 [py_artm.quantities.Perplexity(), py_artm.quantities.TopicsLeft(),\n",
      "                                  py_artm.stop_conditions.ValueBounds('topics_left', lo=2)])\n",
      "em.iterate(1000, quiet=1)\n",
      "pwt = em.phi\n",
      "ptd = em.theta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "\u0417\u0430\u043f\u0438\u0441\u044c \u043d\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u043a\u0430\u043a \u041d\u0430\u0434\u0438\u043d\u044b\u0445"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('MMRO_new.txt', 'w') as nwdf:\n",
      "    new_nwd = nwd.tocsr()[np.nonzero(nwd.sum(1).A1)[0], :].tocoo()\n",
      "    \n",
      "    nwdf.write(str(new_nwd.shape[1]) + '\\n')\n",
      "    nwdf.write(str(new_nwd.shape[0]) + '\\n')\n",
      "    \n",
      "    for w, d, cnt in zip(new_nwd.row, new_nwd.col, new_nwd.data):\n",
      "        nwdf.write('%d %d %d' % (d, w, cnt) + '\\n')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with codecs.open('dictionary_new.txt', mode='w', encoding='utf-8') as dictf:\n",
      "    words_new = words[np.nonzero(nwd.sum(1).A1)[0]]\n",
      "    dictf.write(str(words_new.shape[0]) + '\\n')\n",
      "    for i, word in enumerate(words_new):\n",
      "        dictf.write('%d %s\\n' % (i, word.upper()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 94
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "\u0421\u043e\u043f\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0438\u043c\u0451\u043d \u0444\u0430\u0439\u043b\u043e\u0432 - \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u044d\u0442\u0430\u043f\u043e\u0432"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('filenames.txt') as fnamesf:\n",
      "    docfnames = [line.strip().split(' ', 1) for line in fnamesf]\n",
      "    docfnames = [(int(d), fname) for d, fname in docfnames]\n",
      "    d_to_fname = {d: fname for d, fname in docfnames}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('filenames.txt') as fnamesf:\n",
      "    docfnames = [line.strip().split(' ', 1)[1] for line in fnamesf]\n",
      "    docfiles = [(fname, open('documents/' + fname).read()) for fname in docfnames]\n",
      "\n",
      "full_xml_files = [(fname, open(fname).read()) for fname in glob('documents/xml/*/*')]\n",
      "fname_to_xml = {}\n",
      "\n",
      "for fname, fcont in docfiles:\n",
      "    matches = [fn for fn, fc in full_xml_files if fc == fcont]\n",
      "    if len(matches) == 1:\n",
      "        fname_to_xml[fname] = matches[0].replace('documents/xml/XML_', '').replace('.xml', '')\n",
      "    else:\n",
      "        print fname, matches"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xml_to_tex = {}\n",
      "\n",
      "tex_fnames = glob('documents/tex/*/*')\n",
      "for xml_fname in glob('documents/xml/*/*'):\n",
      "    fname = xml_fname.replace('/xml/', '/tex/').replace('/XML_', '/').replace('.xml', '.tex').replace('unequated_', '')\n",
      "    matches = [tex_fname for tex_fname in tex_fnames if fname.lower() == tex_fname.lower()]\n",
      "    if len(matches) == 1:\n",
      "        xml_to_tex[xml_fname.replace('documents/xml/XML_', '').replace('.xml', '')] = matches[0].replace('documents/tex/', '')\n",
      "    else:\n",
      "        print xml_fname, matches"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "documents/xml/XML_2007-MMRO13/unequated_DeWansa_SI_1 - copy.xml []\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d_to_tex = {}\n",
      "\n",
      "for d in range(1009):\n",
      "    fname = d_to_fname.get(d, None)\n",
      "    full = fname_to_xml.get(fname, None)\n",
      "    tex = xml_to_tex.get(full, None)\n",
      "    if tex is None:\n",
      "        print d, fname, full, tex\n",
      "    else:\n",
      "        d_to_tex[d] = tex"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "391 unequated_DeWansa_SI_1 - copy.xml 2007-MMRO13/unequated_DeWansa_SI_1 - copy None\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d_to_html = {}\n",
      "\n",
      "for d, tex in d_to_tex.items():\n",
      "    d_to_html[d] = re.sub('.tex$', '', tex, flags=re.I)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('filenames_new.txt', 'w') as f:\n",
      "    for d, tex in sorted(d_to_html.items()):\n",
      "        f.write('%d %s\\n' % (d, tex))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "\u041a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0442\u0435\u0445\u043e\u0432 \u0432 \u0445\u0442\u043c\u043b \u0438 \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0430"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for fname in ProgressBar(glob('documents/tex/*/*')):\n",
      "    outfname = fname.replace('/tex/', '/html/') + '.htex'\n",
      "    outdir = outfname[::-1].split('/', 1)[1][::-1]\n",
      "    print fname, '->', outfname\n",
      "    !mkdir -p {outdir}\n",
      "    !rm {outfname}\n",
      "    !iconv -f cp1251 -t utf-8 {fname} | perl -pe '/^[^%]*%%/ && next; /\\\\%/ && next; s/%.*\\n//g' | pandoc -f latex -t html5 -o {outfname} --standalone --gladtex"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for fname in ProgressBar(glob('documents/html/*/*.htex')):\n",
      "    outfname = re.sub('.tex.htex$', '.html', fname, flags=re.I)\n",
      "    print fname, '->', outfname\n",
      "    !gladtex -d {outfname.replace('.html', '')} {fname}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "htexs = glob('documents/html/*/*.htex')\n",
      "htmls = glob('documents/html/*/*.html')\n",
      "emptyfnames = !find documents -type f -empty\n",
      "\n",
      "for texfname in glob('documents/tex/*/*'):\n",
      "    htexfname = texfname.replace('/tex/', '/html/') + '.htex'\n",
      "    htmlfname = re.sub('.tex.htex$', '.html', htexfname, flags=re.I)\n",
      "    ishtex = htexfname in htexs\n",
      "    ishtml = htmlfname in htmls\n",
      "    isempty = (htexfname in emptyfnames) or (htmlfname in emptyfnames)\n",
      "\n",
      "    if not ishtex or not ishtml or isempty:\n",
      "        print '-+'[ishtex], '-+'[ishtml], '+-'[isempty], texfname\n",
      "#         !iconv -f cp1251 -t utf-8 '{texfname}' | sed 's/\\r$//g' | sed 's/\\r/\\n/g' | sed -e '/.$/a\\' | perl -pe '/\\\\%/ && next; s/%.*\\n//g' | pandoc -f latex -t html5 -o '{htexfname}' --standalone --gladtex\n",
      "#         !gladtex -v -d '{htmlfname.replace('.html', '')}' '{htexfname}'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "csses = !ls web/static/docsdata/*/*.css\n",
      "for css in ProgressBar(csses):\n",
      "    parts = css.split('/')\n",
      "    repl = '/static/docsdata/%s/%s' % (parts[3], parts[4].replace('.css', '.png'))\n",
      "    !sed -i \"s|display: inline-block;||g\" '{css}'\n",
      "    !sed -i \"s|url('.*|url('{repl}');|g\" '{css}'\n",
      "    !sed -i \"s|background-image|display: inline-block; \\nbackground-image|g\" '{css}'\n",
      "#     !cat {css}\n",
      "#     break"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "\u041e\u043f\u0435\u0440\u0430\u0446\u0438\u0438 \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0432 HDF"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with h5py.File('data.hdf', mode='r') as h5f:    \n",
      "    docinfos = []\n",
      "    \n",
      "    for d in range(len(h5f['filenames'])):\n",
      "        filename = h5f['filenames'][d]\n",
      "        if filename != '0':\n",
      "            year, conference = filename.split('/')[0].split('-')\n",
      "        else:\n",
      "            year = 9999\n",
      "            conference = 'UNKNOWN99'\n",
      "        conference, confnum = re.match(r'^(\\D+)(\\d+)$', conference).groups()\n",
      "        year = int(year)\n",
      "        confnum = int(confnum)\n",
      "\n",
      "        authors, name = h5f['docinfo'][d]\n",
      "        if name in ['*', '**']:\n",
      "            name = 'UNKNOWN'\n",
      "\n",
      "        slug = re.search('\\w+', authors, re.U).group(0) + str(year)[2:] + re.search('\\w+', name, re.U).group(0)\n",
      "        slug = slug.lower()\n",
      "        \n",
      "        docinfos.append((conference, confnum, year, authors, name, slug))\n",
      "        \n",
      "    docinfos = np.array(docinfos, dtype=[\n",
      "        ('conference', h5py.special_dtype(vlen=str)),\n",
      "        ('confnum', np.int32),\n",
      "        ('year', np.int32),\n",
      "        ('authors', h5py.special_dtype(vlen=unicode)),\n",
      "        ('name', h5py.special_dtype(vlen=unicode)),\n",
      "        ('slug', h5py.special_dtype(vlen=unicode)),\n",
      "    ])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "too many values to unpack",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-35-345cb6f74e4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mconfnum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mauthors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'docinfo'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'*'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'**'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'UNKNOWN'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mValueError\u001b[0m: too many values to unpack"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with h5py.File('data.hdf', mode='w') as f:\n",
      "    f.create_dataset('p_wt', data=pwt, compression='lzf')\n",
      "    f.create_dataset('p_td', data=ptd, compression='lzf')\n",
      "    f.create_dataset('p_wd', data=np.dot(pwt, ptd), compression='lzf')\n",
      "    f.create_dataset('n_wd', data=nwd.A, dtype=np.int32, compression='lzf')\n",
      "    f.create_dataset('dictionary', data=words, compression='lzf')\n",
      "    f.create_dataset('docinfo', data=docinfos, compression='lzf')\n",
      "    f.create_dataset('filenames', data=filenames, compression='lzf')\n",
      "    \n",
      "    docs_group = f.create_group('documents')\n",
      "    for i, (topics, doc) in enumerate(docs):\n",
      "        dset = docs_group.create_dataset('%d' % i, data=doc, compression='lzf')\n",
      "        dset.attrs['topics'] = topics"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with h5py.File('data_.hdf', mode='w') as h5f:\n",
      "    with h5py.File('data.hdf', mode='r') as h5f_s:\n",
      "        h5f.create_dataset('p_wt', data=h5f_s['p_wt'], compression='lzf')\n",
      "        h5f.create_dataset('p_td', data=h5f_s['p_td'], compression='lzf')\n",
      "        h5f.create_dataset('p_wd', data=h5f_s['p_wd'], compression='lzf')\n",
      "        h5f.create_dataset('n_wd', data=h5f_s['n_wd'], compression='lzf')\n",
      "        h5f.create_dataset('dictionary', data=h5f_s['dictionary'], compression='lzf')\n",
      "        h5f.create_dataset('docinfo', data=h5f_s['docinfo'], compression='lzf')\n",
      "        h5f.create_dataset('filenames', data=h5f_s['filenames'], compression='lzf')\n",
      "\n",
      "        docs_group = h5f.create_group('documents')\n",
      "        for i, doc in h5f_s['documents'].items():\n",
      "            dset = docs_group.create_dataset(i, data=doc, compression='lzf')\n",
      "            dset.attrs['topics'] = doc.attrs['topics']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!mv data_.hdf data.hdf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "\u041c\u043e\u044f \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$p(t | d, w) = \\frac{p(t, d, w)}{p(d, w)} = \\frac{p(w, t | d) p(d)}{p(w, d)} = \\frac{p(w | t, d) p(t | d) p(d)}{p(w, d)} = \\frac{p(w | t) p(t | d) p(d)}{p(w, d)} = \\frac{p(w | t) p(t | d) p(d)}{p(w | d) p(d)} = \\frac{p(w | t) p(t | d)}{p(w | d)}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$p(t | d, w) = p(w | t) p(t | d)$$\n",
      "$$\\sum_{d,w} p(t | d, w) \\frac{n_{wd}}{n_d} p(d) = \\sum_{d,w} p(t | d, w) p(w | d) p(d) = \\sum_{d,w} p(t | d, w) p(w, d) = \\sum_{d,w} p(t | d, w) \\frac{n_{wd}}{n} = p(t) = \\sum_d p(t | d) p(d) = \\sum_d p(t | d) \\frac{n_d}{n}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$p(t | w) = \\frac{p(t, w)}{p(w)} = \\frac{p(w | t) p(t)}{p(w)} = \\frac{p(w | t) p(t)}{\\sum_d p(w|d) p(d)}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u041f\u0440\u043e\u0432\u0435\u0440\u0435\u043d\u043e:\n",
      "$$p(t | d) \\approx \\sum_w p(t | d, w) p(w | d)$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with h5py.File('data.hdf', mode='r') as h5f:\n",
      "    pwt = h5f['p_wt'][...] # p(w | t)\n",
      "    ptd = h5f['p_td'][...] # p(t | d)\n",
      "    raise\n",
      "    nwd = h5f['n_wd'][...] # n_{wd}\n",
      "    nw = nwd.sum(1) # n_w\n",
      "    nd = nwd.sum(0) # n_d\n",
      "    n = nd.sum() # n\n",
      "    pw = 1.0 * nw / n # p(w) = n_w / n\n",
      "    pd = 1.0 * nd / n # p(d) = n_d / n\n",
      "    pt = ptd.dot(pd) # p(t) = \\sum_d p(t | d) p(d)\n",
      "    pwd = 1.0 * nwd / nd # p(w | d) = n_{wd} / n_d\n",
      "#     p_wd = 1.0 * nwd / n # p(w, d) = n_{wd} / n\n",
      "#     for t in range(50):\n",
      "#         ptdw = (pwt[:, t] * ptd[t, :][:, np.newaxis]).T # p(t | d, w) = p(w | t) p(t | d)\n",
      "#         print (ptdw * pwd).sum(), pt[t]\n",
      "    for d in range(1009):\n",
      "        pwts = pwt * ptd[:, d].T\n",
      "        pwts /= pwts.sum(1)[:,np.newaxis]\n",
      "        pwts[np.isnan(pwts.sum(1)), :] = 0\n",
      "        pandas.Series(ptd[:,d] - pwts.T.dot(pwd[:, d])).plot()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "exceptions must be old-style classes or derived from BaseException, not NoneType",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-7-00ef0bfe7f83>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mpwt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'p_wt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# p(w | t)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mptd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'p_td'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# p(t | d)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mnwd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'n_wd'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# n_{wd}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mnw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnwd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# n_w\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mTypeError\u001b[0m: exceptions must be old-style classes or derived from BaseException, not NoneType"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class MyHTMLParser(HTMLParser):\n",
      "    \n",
      "    def __init__(self, html):\n",
      "        HTMLParser.__init__(self)\n",
      "\n",
      "        self.html = html\n",
      "        \n",
      "        self.morph = pymorphy2.MorphAnalyzer()\n",
      "        # https://pymorphy2.readthedocs.org/en/latest/user/grammemes.html\n",
      "        self.ignore_tags = ['LATN', 'PNCT', 'NUMB', 'ROMN', 'UNKN',\n",
      "                            'PREP', 'CONJ', 'PRCL', 'INTJ', 'NPRO', 'NUMR', 'Abbr']\n",
      "        self.minwlen = 4\n",
      "\n",
      "        self.lineslens = map(lambda l: len(l) + 1, self.html.split('\\n'))\n",
      "        \n",
      "        self.inheader = None\n",
      "        self.metadata = defaultdict(str)\n",
      "        \n",
      "        self.words = []\n",
      "        self.cntlatin = 0\n",
      "        self.started = False\n",
      "        \n",
      "        self.feed(self.html)\n",
      "    \n",
      "    def handle_starttag(self, tag, attrs):\n",
      "        attrs = dict(attrs)\n",
      "        if tag == 'body':\n",
      "            self.started = True\n",
      "        elif len(tag) == 2 and tag[0] == 'h' and 'class' in attrs:\n",
      "            self.inheader = attrs['class']\n",
      "            \n",
      "    def handle_endtag(self, tag):\n",
      "        if len(tag) == 2 and tag[0] == 'h':\n",
      "            if self.inheader:\n",
      "                self.metadata[self.inheader] = self.metadata[self.inheader].strip()\n",
      "            self.inheader = None\n",
      "        \n",
      "    def handle_data(self, data):\n",
      "        if self.inheader:\n",
      "            self.metadata[self.inheader] += data\n",
      "        \n",
      "        lineno, loffset = self.getpos()\n",
      "        data_offset = sum(self.lineslens[:lineno-1]) + loffset\n",
      "        sdata = self.html[data_offset: data_offset+len(data)]\n",
      "        assert data == sdata, (data, sdata)\n",
      "        \n",
      "        matches = re.finditer(r'\\b\\w+\\b', data, flags=re.U)\n",
      "        for m in matches:\n",
      "            word = m.group(0)\n",
      "            if len(word) < self.minwlen:\n",
      "                continue\n",
      "\n",
      "            start = data_offset + m.start()\n",
      "            end = data_offset + m.end()\n",
      "            assert word == self.html[start:end]\n",
      "            \n",
      "            wparsed = self.morph.parse(word)[0]\n",
      "            wnorm = wparsed.normal_form\n",
      "            if 'LATN' in wparsed.tag:\n",
      "                self.cntlatin += 1\n",
      "            if not any(ign in wparsed.tag for ign in self.ignore_tags):\n",
      "                self.words.append((word, wnorm, start, end))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "docs = []\n",
      "\n",
      "for fname in ProgressBar(glob('web/static/docsdata/*/*.html')):\n",
      "    with codecs.open(fname, encoding='utf-8') as f:\n",
      "        html = f.read()\n",
      "\n",
      "        parser = MyHTMLParser(html)\n",
      "        doc_content = np.array(parser.words, dtype=[('word', h5py.special_dtype(vlen=unicode)),\n",
      "                                                      ('wnorm', h5py.special_dtype(vlen=unicode)),\n",
      "                                                      ('start', np.int32),\n",
      "                                                      ('end', np.int32)])\n",
      "\n",
      "        docs.append((fname, parser.metadata, parser.cntlatin, doc_content))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for f, t, _, _ in docs:\n",
      "    if len(t) != 2:\n",
      "        print t.keys()\n",
      "#     print t['title']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# latin words counts\n",
      "fracs = pd.Series([1.0 * cntlatin / (cntlatin + len(content)) for fname, meta, cntlatin, content in docs])\n",
      "fracs.hist(bins=100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "docs = [(fname, meta, content)\n",
      "        for fname, meta, cntlatin, content in docs\n",
      "        if 1.0 * cntlatin / (cntlatin + len(content)) < 0.5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wnorms = np.concatenate([dc[2]['wnorm'] for dc in docs])\n",
      "wnorms = np.unique(wnorms)\n",
      "dictionary = wnorms.astype(h5py.special_dtype(vlen=unicode))\n",
      "wnorms_dict = {wnorm: w for w, wnorm in enumerate(wnorms)}\n",
      "\n",
      "new_docs = []\n",
      "for fname, meta, content in docs:\n",
      "    new_content = np.empty((len(content),), dtype=[('word', h5py.special_dtype(vlen=unicode)),\n",
      "                                                  ('w', np.int32),\n",
      "                                                  ('start', np.int32),\n",
      "                                                  ('end', np.int32)])\n",
      "    new_content['word'] = content['word']\n",
      "    new_content['start'] = content['start']\n",
      "    new_content['end'] = content['end']\n",
      "    new_content['w'] = [wnorms_dict[wnorm] for wnorm in content['wnorm'].astype('U')]\n",
      "    \n",
      "    new_docs.append((fname, meta, new_content))\n",
      "    \n",
      "docs = new_docs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nwd = np.zeros((len(dictionary), len(docs)), dtype=np.int32)\n",
      "for d, (_, _, content) in enumerate(docs):\n",
      "    for w in content['w']:\n",
      "        nwd[w, d] += 1\n",
      "\n",
      "nwd_sp = scipy.sparse.coo_matrix(nwd)\n",
      "nwd_sp_arr = np.array(map(tuple, np.vstack([nwd_sp.data, nwd_sp.col, nwd_sp.row]).T),\n",
      "                      dtype=[('data', np.int32),\n",
      "                             ('col', np.int32),\n",
      "                             ('row', np.int32)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "metadata = []\n",
      "for fname, meta, cont in new_docs:\n",
      "    fname = fname.replace('web/static/docsdata/', '').replace('.html', '')\n",
      "\n",
      "    year, conference = fname.split('/')[-2].split('-')\n",
      "    conference, confnum = re.match(r'^(\\D+)(\\d+)$', conference).groups()\n",
      "    year = int(year)\n",
      "    confnum = int(confnum)\n",
      "    \n",
      "    author_fw = re.search(r'\\w{3,}', meta['author'], re.U).group(0)\n",
      "    title_fw = re.search(r'\\w{3,}', meta['title'], re.U).group(0)\n",
      "    slug = author_fw + str(year)[2:] + title_fw\n",
      "    slug = slug.lower()\n",
      "\n",
      "    metadata.append((fname, meta['title'], meta['author'], slug, conference, confnum, year))\n",
      "    \n",
      "metadata = np.array(metadata, dtype=[('filename', h5py.special_dtype(vlen=unicode)),\n",
      "                                     ('title', h5py.special_dtype(vlen=unicode)),\n",
      "                                     ('author', h5py.special_dtype(vlen=unicode)),\n",
      "                                     ('slug', h5py.special_dtype(vlen=unicode)),\n",
      "                                     ('conference', h5py.special_dtype(vlen=unicode)),\n",
      "                                     ('confnum', np.int32),\n",
      "                                     ('year', np.int32)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "t_init = 50\n",
      "\n",
      "em = py_artm.plsa.PlsaEmRational(nwd_sp.astype(np.float32),\n",
      "                                 t_init,\n",
      "                                 [py_artm.quantities.Perplexity(), py_artm.quantities.TopicsLeft(),\n",
      "                                  py_artm.stop_conditions.ValueBounds('topics_left', lo=2)])\n",
      "em.iterate(2000, quiet=2)\n",
      "pwt = em.phi\n",
      "ptd = em.theta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "docs_contents = []\n",
      "\n",
      "for d, (_, _, content) in enumerate(docs):\n",
      "    pwts = pwt * ptd[:, d].T\n",
      "    pwts /= pwts.sum(1)[:,np.newaxis]\n",
      "    pwts[np.isnan(pwts.sum(1)), :] = 0\n",
      "\n",
      "    topics_glob = ptd[:,d].argsort()[::-1][:np.count_nonzero(ptd[:,d])]\n",
      "    doc_content = []\n",
      "\n",
      "    for word, w, start, end in content:\n",
      "        pts = pwts[w]\n",
      "        topics = pts.argsort()[::-1][:5]\n",
      "        pts_glob = pts[topics_glob]\n",
      "        pts = pts[topics]\n",
      "\n",
      "        doc_content.append((w, start, end, topics, pts, pts_glob))\n",
      "\n",
      "    docs_contents.append((topics_glob,\n",
      "                         np.array(doc_content,\n",
      "                                 dtype=[('w', np.int32),\n",
      "                                        ('startpos', np.int32),\n",
      "                                        ('endpos', np.int32),\n",
      "                                        ('topics', np.int32, 5),\n",
      "                                        ('pts', np.float32, 5),\n",
      "                                        ('pts_glob', np.float32, len(topics_glob))])))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with h5py.File('data_.hdf', mode='w') as f:\n",
      "    f.create_dataset('p_wt', data=pwt, compression='lzf')\n",
      "    f.create_dataset('p_td', data=ptd, compression='lzf')\n",
      "    f.create_dataset('n_wd_coo', data=nwd_sp_arr, compression='lzf')\n",
      "    f.create_dataset('dictionary', data=dictionary, compression='lzf')\n",
      "    f.create_dataset('metadata', data=metadata, compression='lzf')\n",
      "    \n",
      "    docs_group = f.create_group('documents')\n",
      "    for i, (topics, doc) in enumerate(docs_contents):\n",
      "        dset = docs_group.create_dataset('%d' % i, data=doc, compression='lzf')\n",
      "        dset.attrs['topics'] = topics"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!mv data_.hdf data.hdf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from whoosh import fields, analysis, index, qparser, scoring, query, spelling, automata, highlight, sorting, formats\n",
      "import h5py, codecs, re"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class RemoveDuplicatesFilter(analysis.Filter):\n",
      "    def __call__(self, stream):\n",
      "        lasttext = None\n",
      "        for token in stream:\n",
      "            if lasttext != token.text:\n",
      "                yield token\n",
      "            lasttext = token.text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "analyzer = (analysis.RegexTokenizer(r'\\S+') |\n",
      "            analysis.MultiFilter(index=analysis.IntraWordFilter(mergewords=True, mergenums=True), query=analysis.IntraWordFilter()) |\n",
      "            analysis.SubstitutionFilter(r'^\\W+', '') |\n",
      "            analysis.SubstitutionFilter(r'\\W+$', '') |\n",
      "            analysis.StopFilter(lang='ru') |\n",
      "            analysis.TeeFilter(analysis.PassFilter(),\n",
      "                               analysis.LowercaseFilter(),\n",
      "                               analysis.StemFilter(lang='ru')) |\n",
      "            analysis.StopFilter(stoplist=set()) |\n",
      "            RemoveDuplicatesFilter())\n",
      "\n",
      "topics_analyser = (analysis.RegexTokenizer(r'\\S+') |\n",
      "                   analysis.DelimitedAttributeFilter(attribute='boost', default=float('nan'), type=float))\n",
      "\n",
      "topics_field = fields.FieldType(formats.PositionBoosts(), \n",
      "                                analyzer=topics_analyser, \n",
      "                                stored=True, \n",
      "                                vector=formats.PositionBoosts())\n",
      "\n",
      "schema = fields.Schema(d=fields.ID(stored=True),\n",
      "                       fname=fields.ID(stored=True),\n",
      "                       slug=fields.ID(stored=True),\n",
      "                       topics=topics_field,\n",
      "                       content=fields.TEXT(stored=True, spelling=True, phrase=True, chars=True, analyzer=analyzer),\n",
      "                       title=fields.TEXT(stored=True, spelling=True, analyzer=analyzer, field_boost=3.0),\n",
      "                       authors=fields.TEXT(stored=True, spelling=True, field_boost=2.0),\n",
      "                       conference=fields.ID(stored=True),\n",
      "                       year=fields.NUMERIC(stored=True),\n",
      "                       \n",
      "                       title_ngrams=fields.NGRAMWORDS(field_boost=2.0),\n",
      "                       authors_ngrams=fields.NGRAMWORDS(field_boost=2.0),\n",
      "                       authors_tags=fields.KEYWORD(stored=True, commas=True))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 228
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!rm -rf whoosh_ix\n",
      "!mkdir whoosh_ix\n",
      "ix = index.create_in('whoosh_ix', schema)\n",
      "!ls whoosh_ix -lh"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "total 32K\r\n",
        "-rw-r--r-- 1 alexander alexander 31K Sep 22 09:55 _MAIN_0.toc\r\n"
       ]
      }
     ],
     "prompt_number": 229
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ix = index.open_dir('whoosh_ix', readonly=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "with ix.writer() as writer, h5py.File('data.hdf', mode='r') as h5f:\n",
      "    ptd = h5f['p_td'][...]\n",
      "    \n",
      "    for d, (fname, title, author, slug, conf, _, year) in enumerate(h5f['metadata'][...]):\n",
      "        with codecs.open('web/static/docsdata/%s.html' % fname, encoding='utf-8') as f:\n",
      "            html = f.read()\n",
      "            m = re.search(r'<body>.*</body>', html, re.DOTALL)\n",
      "            if m is None:\n",
      "                continue\n",
      "            html = m.group(0)\n",
      "            content = re.sub('<[^<]+?>', ' ', html)\n",
      "            \n",
      "        topics = ptd[:,d].argsort()[::-1][:np.count_nonzero(ptd[:,d])]\n",
      "        topics = ' '.join('%s^%s' % (t, p) for t, p in zip(topics, ptd[topics,d]))\n",
      "            \n",
      "        data = {\n",
      "            'd': unicode(d),\n",
      "            'fname': unicode(fname),\n",
      "            'slug': unicode(slug),\n",
      "            'content': unicode(content),\n",
      "            'title': unicode(title),\n",
      "            'authors': unicode(author),\n",
      "            'conference': unicode(conf),\n",
      "            'year': year,\n",
      "            'topics': unicode(topics)\n",
      "        }\n",
      "        docdata = {name: data[name.split('_')[0]]\n",
      "                   for name in schema.names()}\n",
      "        writer.add_document(**docdata)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 2min 12s, sys: 1.14 s, total: 2min 13s\n",
        "Wall time: 2min 13s\n"
       ]
      }
     ],
     "prompt_number": 230
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reload(highlight)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 58,
       "text": [
        "<module 'whoosh.highlight' from '/usr/local/lib/python2.7/dist-packages/whoosh/highlight.py'>"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "qp = qparser.MultifieldParser(['content', 'title', 'authors', 'authors_ngrams', 'title_ngrams'],\n",
      "                              schema=ix.schema,\n",
      "                              termclass=query.FuzzyTerm)\n",
      "\n",
      "# highlighter_whole = highlight.Highlighter(fragmenter=highlight.WholeFragmenter())\n",
      "highlighter_whole = highlight.Highlighter(fragmenter=highlight.PinpointFragmenter(surround=50, maxchars=1000))\n",
      "\n",
      "with ix.searcher() as searcher:\n",
      "    qtext = u'\u0441\u0442\u0440\u0438\u0436\u043e'\n",
      "    q = qp.parse(qtext)\n",
      "    print unicode(q)\n",
      "    \n",
      "#     corr = searcher.correct_query(q, qtext)\n",
      "#     print unicode(corr.query)\n",
      "#     print corr.string\n",
      "#     print corr.format_string(highlight.HtmlFormatter())\n",
      "#     q = corr.query\n",
      "    \n",
      "    results = searcher.search(q, terms=True)\n",
      "    results.highlighter = highlighter_whole\n",
      "    \n",
      "    for hit in results[:2]:\n",
      "        print hit['authors'], hit['title']\n",
      "        hl = hit.highlights('content')\n",
      "        print\n",
      "        print hl\n",
      "#         print hit['content'][31:]\n",
      "        print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "((content:\u0441\u0442\u0440\u0438\u0436\u043e~ AND content:\u0441\u0442\u0440\u0438\u0436~) OR (title:\u0441\u0442\u0440\u0438\u0436\u043e~ AND title:\u0441\u0442\u0440\u0438\u0436~) OR authors:\u0441\u0442\u0440\u0438\u0436\u043e~ OR (authors_ngrams:\u0441\u0442\u0440\u0438 AND authors_ngrams:\u0442\u0440\u0438\u0436 AND authors_ngrams:\u0440\u0438\u0436\u043e) OR (title_ngrams:\u0441\u0442\u0440\u0438 AND title_ngrams:\u0442\u0440\u0438\u0436 AND title_ngrams:\u0440\u0438\u0436\u043e))\n",
        "\u0421\u0442\u0440\u0438\u0436\u043e\u0432\u00a0\u0412.\u0412., \u041f\u0442\u0430\u0448\u043a\u043e\u00a0\u0413.\u041e."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \u041f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u0438\u043d\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043e\u0432 \u043d\u0430\u00a0\u043c\u043d\u043e\u0436\u0435\u0441\u0442\u0432\u0435 \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u0440\u044f\u0434\u043e\u0432 \u043f\u0443\u0442\u0435\u043c \u0434\u0438\u043d\u0430\u043c\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0441\u0432\u0435\u0440\u0442\u043a\u0438 \u0441\u0432\u043e\u0431\u043e\u0434\u043d\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439\n",
        "\n",
        "\u043f\u0443\u0442\u0435\u043c \u0434\u0438\u043d\u0430\u043c\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0441\u0432\u0435\u0440\u0442\u043a\u0438 \u0441\u0432\u043e\u0431\u043e\u0434\u043d\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439 \n",
        " <b class=\"match term0\">\u0421\u0442\u0440\u0438\u0436\u043e\u0432</b>\u00a0\u0412.\u0412., \u041f\u0442\u0430\u0448\u043a\u043e\u00a0\u0413.\u041e. \n",
        " \n",
        " \u0414\u0430\u043d\u043e \u043c\u043d\u043e\u0436\u0435\u0441\u0442\u0432\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u0440\u044f\n",
        "\n",
        "\u0420\u0443\u0434\u043e\u0439\u00a0\u0413.\u0418., \u0421\u0442\u0440\u0438\u0436\u043e\u0432\u00a0\u0412.\u0412. \u0423\u043f\u0440\u043e\u0449\u0435\u043d\u0438\u0435 \u0441\u0443\u043f\u0435\u0440\u043f\u043e\u0437\u0438\u0446\u0438\u0439 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u0430\u0440\u043d\u044b\u0445 \u0444\u0443\u043d\u043a\u0446\u0438\u0439 \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0439 \u0433\u0440\u0430\u0444\u043e\u0432 \u043f\u043e \u043f\u0440\u0430\u0432\u0438\u043b\u0430\u043c\n",
        "\n",
        "\u0438 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0439 \u0433\u0440\u0430\u0444\u043e\u0432 \u043f\u043e \u043f\u0440\u0430\u0432\u0438\u043b\u0430\u043c \n",
        " \u0420\u0443\u0434\u043e\u0439\u00a0\u0413.\u0418., <b class=\"match term0\">\u0421\u0442\u0440\u0438\u0436\u043e\u0432</b>\u00a0\u0412.\u0412. \n",
        " \n",
        " [\u0420\u0443\u0434\u043e\u0439\u00a0\u0413.\u0418.  , <b class=\"match term0\">\u0421\u0442\u0440\u0438\u0436\u043e\u0432</b>\u00a0\u0412.\u0412.  ]   1    email  rudoy@forecsys.ru     organ\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 60
    }
   ],
   "metadata": {}
  }
 ]
}
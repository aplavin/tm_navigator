{
 "metadata": {
  "name": "",
  "signature": "sha256:2c853e281d2db239e2f0f20decff918b64a21c72c344c242cc5190d0ce53f016"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import codecs\n",
      "import numpy as np\n",
      "import pandas\n",
      "import scipy.sparse\n",
      "from scipy.ndimage.filters import convolve1d\n",
      "import sys\n",
      "import comp_exp\n",
      "import py_artm\n",
      "import h5py\n",
      "sys.path.append('/home/alexander/strijov/MLAlgorithms/Group174/Plavin2014TopicsNumberOptimization/code')\n",
      "from utils import *\n",
      "from ggplot_utils import *\n",
      "from IPython.display import display, display_html\n",
      "from functools import partial\n",
      "from itertools import count, groupby\n",
      "from ipy_progressbar import ProgressBar\n",
      "from itertools import starmap\n",
      "import codecs\n",
      "from pprint import pprint\n",
      "from collections import Counter, defaultdict\n",
      "from recordtype import recordtype\n",
      "import re\n",
      "from glob import glob\n",
      "from HTMLParser import HTMLParser\n",
      "from htmlentitydefs import name2codepoint\n",
      "import pymorphy2\n",
      "\n",
      "from rpy2.robjects import pandas2ri\n",
      "pandas2ri.activate()\n",
      "import rpy2.robjects as robj\n",
      "from rpy2.robjects.lib import ggplot2 as gg\n",
      "from rpy2.ipython.ggplot import ggplot as iggplot\n",
      "from rpy2.robjects.packages import importr\n",
      "rbase = importr('base')\n",
      "\n",
      "%matplotlib inline\n",
      "%load_ext rpy2.ipython.rmagic\n",
      "\n",
      "from ggplot_utils import *\n",
      "MyGGPlot.latex_enabled = False"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "\u0421\u0447\u0438\u0442\u044b\u0432\u0430\u043d\u0438\u0435 \u041d\u0430\u0434\u0438\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ds = []\n",
      "ws = []\n",
      "cnts = []\n",
      "\n",
      "with open('MMRO_new.txt') as nwdf:\n",
      "    D = int(nwdf.readline())\n",
      "    W = int(nwdf.readline())\n",
      "    \n",
      "    for line in nwdf:\n",
      "        d, w, cnt = map(int, line.split())\n",
      "        ds.append(d)\n",
      "        ws.append(w)\n",
      "        cnts.append(cnt)\n",
      "        \n",
      "nwd = scipy.sparse.coo_matrix((cnts, (ws, ds))).astype(np.float32)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nwd.shape, sum(nwd.sum(1).A1 == 0), sum(nwd.sum(0).A1 == 0), 13031-668"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with codecs.open('authors.txt', encoding='utf-8') as authf:\n",
      "    lines = [line.strip() for line in authf]\n",
      "    docinfos = []\n",
      "    i = 0\n",
      "    while i < len(lines):\n",
      "        assert int(lines[i]) * 3 == i\n",
      "        assert int(lines[i]) == len(docinfos)\n",
      "        i += 1\n",
      "        docinfo = []\n",
      "        while i < len(lines) and not lines[i].isdigit():\n",
      "            docinfo.append(lines[i])\n",
      "            i += 1\n",
      "        assert len(docinfo) == 2\n",
      "        docinfos.append(tuple(docinfo))\n",
      "    docinfos = np.array(docinfos)\n",
      "    docinfos = np.array(docinfos, dtype=h5py.special_dtype(vlen=unicode))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with codecs.open('dictionary_new.txt', encoding='utf-8') as dictf:\n",
      "    assert nwd.shape[0] == int(dictf.readline().strip())\n",
      "    indwords = [line.strip().split(' ', 1) for line in dictf]\n",
      "    indwords[723] = ['723', '']\n",
      "    assert all(int(ind) == i for i, (ind, w) in enumerate(indwords))\n",
      "    words = [w.lower() for ind, w in indwords]\n",
      "    words = np.array(words, dtype=h5py.special_dtype(vlen=unicode))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "words_dict = {word: w for w, word in enumerate(words)}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with codecs.open('MMROwithOrder.txt', encoding='utf-8') as docsf:\n",
      "    lines = [line.strip() for line in docsf]\n",
      "    docs = []\n",
      "    i = 0\n",
      "    while i < len(lines):\n",
      "        assert int(lines[i]) == len(docs)\n",
      "        i += 1\n",
      "        d = len(docs)\n",
      "        \n",
      "        pwts = pwt * ptd[:, d].T\n",
      "        pwts /= pwts.sum(1)[:,np.newaxis]\n",
      "        pwts[np.isnan(pwts.sum(1)), :] = 0\n",
      "        \n",
      "        topics_glob = ptd[:,d].argsort()[::-1][:np.count_nonzero(ptd[:,d])]\n",
      "        \n",
      "        doc = []\n",
      "        while i < len(lines) and not lines[i].isdigit():\n",
      "            try:\n",
      "                word, word_norm = lines[i].lower().split(' ', 1)\n",
      "            except ValueError:\n",
      "                word = lines[i].lower().strip()\n",
      "                word_norm = word\n",
      "\n",
      "            # dictionary lookup\n",
      "            try:\n",
      "                w = words_dict[word_norm]\n",
      "            except KeyError:\n",
      "                w = -1\n",
      "                \n",
      "            # topics info\n",
      "            pts = pwts[w]\n",
      "            topics = pts.argsort()[::-1][:5]\n",
      "            pts_glob = pts[topics_glob]\n",
      "            pts = pts[topics]\n",
      "                \n",
      "            doc.append((word, w, topics, pts, pts_glob))\n",
      "            i += 1\n",
      "\n",
      "        docs.append((topics_glob,\n",
      "                     np.array(map(tuple, doc),\n",
      "                             dtype=[('word', h5py.special_dtype(vlen=unicode)),\n",
      "                                    ('w', np.int32),\n",
      "                                    ('topics', np.int32, 5),\n",
      "                                    ('pts', np.float32, 5),\n",
      "                                    ('pts_glob', np.float32, len(topics_glob))])))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('filenames_new.txt') as fnamesf:\n",
      "    docfnames = [line.strip().split(' ', 1) for line in fnamesf]\n",
      "    docfnames = [(int(d), fname) for d, fname in docfnames]\n",
      "    filenames = np.zeros((max(d for d, fname in docfnames) + 1,), dtype=h5py.special_dtype(vlen=str))\n",
      "    for d, fname in docfnames:\n",
      "        filenames[d] = fname"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "\u0417\u0430\u043f\u0443\u0441\u043a \u043c\u043e\u0434\u0435\u043b\u0438"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tau = 0.2\n",
      "t_init = 50\n",
      "\n",
      "em = py_artm.plsa.PlsaEmRational(nwd,\n",
      "                                 t_init,\n",
      "                                 [py_artm.quantities.Perplexity(), py_artm.quantities.TopicsLeft(),\n",
      "                                  py_artm.stop_conditions.ValueBounds('topics_left', lo=2)])\n",
      "em.iterate(1000, quiet=1)\n",
      "pwt = em.phi\n",
      "ptd = em.theta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "\u0417\u0430\u043f\u0438\u0441\u044c \u043d\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u043a\u0430\u043a \u041d\u0430\u0434\u0438\u043d\u044b\u0445"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('MMRO_new.txt', 'w') as nwdf:\n",
      "    new_nwd = nwd.tocsr()[np.nonzero(nwd.sum(1).A1)[0], :].tocoo()\n",
      "    \n",
      "    nwdf.write(str(new_nwd.shape[1]) + '\\n')\n",
      "    nwdf.write(str(new_nwd.shape[0]) + '\\n')\n",
      "    \n",
      "    for w, d, cnt in zip(new_nwd.row, new_nwd.col, new_nwd.data):\n",
      "        nwdf.write('%d %d %d' % (d, w, cnt) + '\\n')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with codecs.open('dictionary_new.txt', mode='w', encoding='utf-8') as dictf:\n",
      "    words_new = words[np.nonzero(nwd.sum(1).A1)[0]]\n",
      "    dictf.write(str(words_new.shape[0]) + '\\n')\n",
      "    for i, word in enumerate(words_new):\n",
      "        dictf.write('%d %s\\n' % (i, word.upper()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 94
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "\u0421\u043e\u043f\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0438\u043c\u0451\u043d \u0444\u0430\u0439\u043b\u043e\u0432 - \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u044d\u0442\u0430\u043f\u043e\u0432"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('filenames.txt') as fnamesf:\n",
      "    docfnames = [line.strip().split(' ', 1) for line in fnamesf]\n",
      "    docfnames = [(int(d), fname) for d, fname in docfnames]\n",
      "    d_to_fname = {d: fname for d, fname in docfnames}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('filenames.txt') as fnamesf:\n",
      "    docfnames = [line.strip().split(' ', 1)[1] for line in fnamesf]\n",
      "    docfiles = [(fname, open('documents/' + fname).read()) for fname in docfnames]\n",
      "\n",
      "full_xml_files = [(fname, open(fname).read()) for fname in glob('documents/xml/*/*')]\n",
      "fname_to_xml = {}\n",
      "\n",
      "for fname, fcont in docfiles:\n",
      "    matches = [fn for fn, fc in full_xml_files if fc == fcont]\n",
      "    if len(matches) == 1:\n",
      "        fname_to_xml[fname] = matches[0].replace('documents/xml/XML_', '').replace('.xml', '')\n",
      "    else:\n",
      "        print fname, matches"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xml_to_tex = {}\n",
      "\n",
      "tex_fnames = glob('documents/tex/*/*')\n",
      "for xml_fname in glob('documents/xml/*/*'):\n",
      "    fname = xml_fname.replace('/xml/', '/tex/').replace('/XML_', '/').replace('.xml', '.tex').replace('unequated_', '')\n",
      "    matches = [tex_fname for tex_fname in tex_fnames if fname.lower() == tex_fname.lower()]\n",
      "    if len(matches) == 1:\n",
      "        xml_to_tex[xml_fname.replace('documents/xml/XML_', '').replace('.xml', '')] = matches[0].replace('documents/tex/', '')\n",
      "    else:\n",
      "        print xml_fname, matches"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d_to_tex = {}\n",
      "\n",
      "for d in range(1009):\n",
      "    fname = d_to_fname.get(d, None)\n",
      "    full = fname_to_xml.get(fname, None)\n",
      "    tex = xml_to_tex.get(full, None)\n",
      "    if tex is None:\n",
      "        print d, fname, full, tex\n",
      "    else:\n",
      "        d_to_tex[d] = tex"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d_to_html = {}\n",
      "\n",
      "for d, tex in d_to_tex.items():\n",
      "    d_to_html[d] = re.sub('.tex$', '', tex, flags=re.I)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('filenames_new.txt', 'w') as f:\n",
      "    for d, tex in sorted(d_to_html.items()):\n",
      "        f.write('%d %s\\n' % (d, tex))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "\u041a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0442\u0435\u0445\u043e\u0432 \u0432 \u0445\u0442\u043c\u043b \u0438 \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0430"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for fname in ProgressBar(glob('documents/tex/*/*')):\n",
      "    outfname = fname.replace('/tex/', '/html/') + '.htex'\n",
      "    outdir = outfname[::-1].split('/', 1)[1][::-1]\n",
      "    print fname, '->', outfname\n",
      "    !mkdir -p {outdir}\n",
      "    !rm {outfname}\n",
      "    !iconv -f cp1251 -t utf-8 {fname} | perl -pe '/^[^%]*%%/ && next; /\\\\%/ && next; s/%.*\\n//g' | pandoc -f latex -t html5 -o {outfname} --standalone --gladtex"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for fname in ProgressBar(glob('documents/html/*/*.htex')):\n",
      "    outfname = re.sub('.tex.htex$', '.html', fname, flags=re.I)\n",
      "    print fname, '->', outfname\n",
      "    !gladtex -d {outfname.replace('.html', '')} {fname}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "htexs = glob('documents/html/*/*.htex')\n",
      "htmls = glob('documents/html/*/*.html')\n",
      "emptyfnames = !find documents -type f -empty\n",
      "\n",
      "for texfname in glob('documents/tex/*/*'):\n",
      "    htexfname = texfname.replace('/tex/', '/html/') + '.htex'\n",
      "    htmlfname = re.sub('.tex.htex$', '.html', htexfname, flags=re.I)\n",
      "    ishtex = htexfname in htexs\n",
      "    ishtml = htmlfname in htmls\n",
      "    isempty = (htexfname in emptyfnames) or (htmlfname in emptyfnames)\n",
      "\n",
      "    if not ishtex or not ishtml or isempty:\n",
      "        print '-+'[ishtex], '-+'[ishtml], '+-'[isempty], texfname\n",
      "#         !iconv -f cp1251 -t utf-8 '{texfname}' | sed 's/\\r$//g' | sed 's/\\r/\\n/g' | sed -e '/.$/a\\' | perl -pe '/\\\\%/ && next; s/%.*\\n//g' | pandoc -f latex -t html5 -o '{htexfname}' --standalone --gladtex\n",
      "#         !gladtex -v -d '{htmlfname.replace('.html', '')}' '{htexfname}'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "csses = !ls web/static/docsdata/*/*.css\n",
      "for css in ProgressBar(csses):\n",
      "    parts = css.split('/')\n",
      "    repl = '/static/docsdata/%s/%s' % (parts[3], parts[4].replace('.css', '.png'))\n",
      "    !sed -i \"s|display: inline-block;||g\" '{css}'\n",
      "    !sed -i \"s|url('.*|url('{repl}');|g\" '{css}'\n",
      "    !sed -i \"s|background-image|display: inline-block; \\nbackground-image|g\" '{css}'\n",
      "#     !cat {css}\n",
      "#     break"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "\u041e\u043f\u0435\u0440\u0430\u0446\u0438\u0438 \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0432 HDF"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with h5py.File('data.hdf', mode='r') as h5f:    \n",
      "    docinfos = []\n",
      "    \n",
      "    for d in range(len(h5f['filenames'])):\n",
      "        filename = h5f['filenames'][d]\n",
      "        if filename != '0':\n",
      "            year, conference = filename.split('/')[0].split('-')\n",
      "        else:\n",
      "            year = 9999\n",
      "            conference = 'UNKNOWN99'\n",
      "        conference, confnum = re.match(r'^(\\D+)(\\d+)$', conference).groups()\n",
      "        year = int(year)\n",
      "        confnum = int(confnum)\n",
      "\n",
      "        authors, name = h5f['docinfo'][d]\n",
      "        if name in ['*', '**']:\n",
      "            name = 'UNKNOWN'\n",
      "\n",
      "        slug = re.search('\\w+', authors, re.U).group(0) + str(year)[2:] + re.search('\\w+', name, re.U).group(0)\n",
      "        slug = slug.lower()\n",
      "        \n",
      "        docinfos.append((conference, confnum, year, authors, name, slug))\n",
      "        \n",
      "    docinfos = np.array(docinfos, dtype=[\n",
      "        ('conference', h5py.special_dtype(vlen=str)),\n",
      "        ('confnum', np.int32),\n",
      "        ('year', np.int32),\n",
      "        ('authors', h5py.special_dtype(vlen=unicode)),\n",
      "        ('name', h5py.special_dtype(vlen=unicode)),\n",
      "        ('slug', h5py.special_dtype(vlen=unicode)),\n",
      "    ])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with h5py.File('data.hdf', mode='w') as f:\n",
      "    f.create_dataset('p_wt', data=pwt, compression='lzf')\n",
      "    f.create_dataset('p_td', data=ptd, compression='lzf')\n",
      "    f.create_dataset('p_wd', data=np.dot(pwt, ptd), compression='lzf')\n",
      "    f.create_dataset('n_wd', data=nwd.A, dtype=np.int32, compression='lzf')\n",
      "    f.create_dataset('dictionary', data=words, compression='lzf')\n",
      "    f.create_dataset('docinfo', data=docinfos, compression='lzf')\n",
      "    f.create_dataset('filenames', data=filenames, compression='lzf')\n",
      "    \n",
      "    docs_group = f.create_group('documents')\n",
      "    for i, (topics, doc) in enumerate(docs):\n",
      "        dset = docs_group.create_dataset('%d' % i, data=doc, compression='lzf')\n",
      "        dset.attrs['topics'] = topics"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with h5py.File('data_.hdf', mode='w') as h5f:\n",
      "    with h5py.File('data.hdf', mode='r') as h5f_s:\n",
      "        h5f.create_dataset('p_wt', data=h5f_s['p_wt'], compression='lzf')\n",
      "        h5f.create_dataset('p_td', data=h5f_s['p_td'], compression='lzf')\n",
      "        h5f.create_dataset('p_wd', data=h5f_s['p_wd'], compression='lzf')\n",
      "        h5f.create_dataset('n_wd', data=h5f_s['n_wd'], compression='lzf')\n",
      "        h5f.create_dataset('dictionary', data=h5f_s['dictionary'], compression='lzf')\n",
      "        h5f.create_dataset('docinfo', data=h5f_s['docinfo'], compression='lzf')\n",
      "        h5f.create_dataset('filenames', data=h5f_s['filenames'], compression='lzf')\n",
      "\n",
      "        docs_group = h5f.create_group('documents')\n",
      "        for i, doc in h5f_s['documents'].items():\n",
      "            dset = docs_group.create_dataset(i, data=doc, compression='lzf')\n",
      "            dset.attrs['topics'] = doc.attrs['topics']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!mv data_.hdf data.hdf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "\u041c\u043e\u044f \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$p(t | d, w) = \\frac{p(t, d, w)}{p(d, w)} = \\frac{p(w, t | d) p(d)}{p(w, d)} = \\frac{p(w | t, d) p(t | d) p(d)}{p(w, d)} = \\frac{p(w | t) p(t | d) p(d)}{p(w, d)} = \\frac{p(w | t) p(t | d) p(d)}{p(w | d) p(d)} = \\frac{p(w | t) p(t | d)}{p(w | d)}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$p(t | d, w) = p(w | t) p(t | d)$$\n",
      "$$\\sum_{d,w} p(t | d, w) \\frac{n_{wd}}{n_d} p(d) = \\sum_{d,w} p(t | d, w) p(w | d) p(d) = \\sum_{d,w} p(t | d, w) p(w, d) = \\sum_{d,w} p(t | d, w) \\frac{n_{wd}}{n} = p(t) = \\sum_d p(t | d) p(d) = \\sum_d p(t | d) \\frac{n_d}{n}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$p(t | w) = \\frac{p(t, w)}{p(w)} = \\frac{p(w | t) p(t)}{p(w)} = \\frac{p(w | t) p(t)}{\\sum_d p(w|d) p(d)}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u041f\u0440\u043e\u0432\u0435\u0440\u0435\u043d\u043e:\n",
      "$$p(t | d) \\approx \\sum_w p(t | d, w) p(w | d)$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with h5py.File('data.hdf', mode='r') as h5f:\n",
      "    pwt = h5f['p_wt'][...] # p(w | t)\n",
      "    ptd = h5f['p_td'][...] # p(t | d)\n",
      "    raise\n",
      "    nwd = h5f['n_wd'][...] # n_{wd}\n",
      "    nw = nwd.sum(1) # n_w\n",
      "    nd = nwd.sum(0) # n_d\n",
      "    n = nd.sum() # n\n",
      "    pw = 1.0 * nw / n # p(w) = n_w / n\n",
      "    pd = 1.0 * nd / n # p(d) = n_d / n\n",
      "    pt = ptd.dot(pd) # p(t) = \\sum_d p(t | d) p(d)\n",
      "    pwd = 1.0 * nwd / nd # p(w | d) = n_{wd} / n_d\n",
      "#     p_wd = 1.0 * nwd / n # p(w, d) = n_{wd} / n\n",
      "#     for t in range(50):\n",
      "#         ptdw = (pwt[:, t] * ptd[t, :][:, np.newaxis]).T # p(t | d, w) = p(w | t) p(t | d)\n",
      "#         print (ptdw * pwd).sum(), pt[t]\n",
      "    for d in range(1009):\n",
      "        pwts = pwt * ptd[:, d].T\n",
      "        pwts /= pwts.sum(1)[:,np.newaxis]\n",
      "        pwts[np.isnan(pwts.sum(1)), :] = 0\n",
      "        pandas.Series(ptd[:,d] - pwts.T.dot(pwd[:, d])).plot()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class MyHTMLParser(HTMLParser):\n",
      "    \n",
      "    def __init__(self, html):\n",
      "        HTMLParser.__init__(self)\n",
      "\n",
      "        self.html = html\n",
      "        \n",
      "        self.morph = pymorphy2.MorphAnalyzer()\n",
      "        # https://pymorphy2.readthedocs.org/en/latest/user/grammemes.html\n",
      "        self.ignore_tags = ['LATN', 'PNCT', 'NUMB', 'ROMN', 'UNKN',\n",
      "                            'PREP', 'CONJ', 'PRCL', 'INTJ', 'NPRO', 'NUMR', 'Abbr']\n",
      "        self.minwlen = 4\n",
      "\n",
      "        self.lineslens = map(lambda l: len(l) + 1, self.html.split('\\n'))\n",
      "        \n",
      "        self.inheader = None\n",
      "        self.metadata = defaultdict(str)\n",
      "        \n",
      "        self.words = []\n",
      "        self.cntlatin = 0\n",
      "        self.started = False\n",
      "        \n",
      "        self.feed(self.html)\n",
      "    \n",
      "    def handle_starttag(self, tag, attrs):\n",
      "        attrs = dict(attrs)\n",
      "        if tag == 'body':\n",
      "            self.started = True\n",
      "        elif len(tag) == 2 and tag[0] == 'h' and 'class' in attrs:\n",
      "            self.inheader = attrs['class']\n",
      "            \n",
      "    def handle_endtag(self, tag):\n",
      "        if len(tag) == 2 and tag[0] == 'h':\n",
      "            if self.inheader:\n",
      "                self.metadata[self.inheader] = self.metadata[self.inheader].strip()\n",
      "            self.inheader = None\n",
      "        \n",
      "    def handle_data(self, data):\n",
      "        if self.inheader:\n",
      "            self.metadata[self.inheader] += data\n",
      "        \n",
      "        lineno, loffset = self.getpos()\n",
      "        data_offset = sum(self.lineslens[:lineno-1]) + loffset\n",
      "        sdata = self.html[data_offset: data_offset+len(data)]\n",
      "        assert data == sdata, (data, sdata)\n",
      "        \n",
      "        matches = re.finditer(r'\\b\\w+\\b', data, flags=re.U)\n",
      "        for m in matches:\n",
      "            word = m.group(0)\n",
      "            if len(word) < self.minwlen:\n",
      "                continue\n",
      "\n",
      "            start = data_offset + m.start()\n",
      "            end = data_offset + m.end()\n",
      "            assert word == self.html[start:end]\n",
      "            \n",
      "            wparsed = self.morph.parse(word)[0]\n",
      "            wnorm = wparsed.normal_form\n",
      "            if 'LATN' in wparsed.tag:\n",
      "                self.cntlatin += 1\n",
      "            if not any(ign in wparsed.tag for ign in self.ignore_tags):\n",
      "                self.words.append((word, wnorm, start, end))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "docs = []\n",
      "\n",
      "for fname in ProgressBar(glob('web/static/docsdata/*/*.html')):\n",
      "    with codecs.open(fname, encoding='utf-8') as f:\n",
      "        html = f.read()\n",
      "\n",
      "        parser = MyHTMLParser(html)\n",
      "        doc_content = np.array(parser.words, dtype=[('word', h5py.special_dtype(vlen=unicode)),\n",
      "                                                      ('wnorm', h5py.special_dtype(vlen=unicode)),\n",
      "                                                      ('start', np.int32),\n",
      "                                                      ('end', np.int32)])\n",
      "\n",
      "        docs.append((fname, parser.metadata, parser.cntlatin, doc_content))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for f, t, _, _ in docs:\n",
      "    if len(t) != 2:\n",
      "        print t.keys()\n",
      "#     print t['title']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# latin words counts\n",
      "fracs = pd.Series([1.0 * cntlatin / (cntlatin + len(content)) for fname, meta, cntlatin, content in docs])\n",
      "fracs.hist(bins=100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "docs = [(fname, meta, content)\n",
      "        for fname, meta, cntlatin, content in docs\n",
      "        if 1.0 * cntlatin / (cntlatin + len(content)) < 0.5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wnorms = np.concatenate([dc[2]['wnorm'] for dc in docs])\n",
      "wnorms = np.unique(wnorms)\n",
      "dictionary = wnorms.astype(h5py.special_dtype(vlen=unicode))\n",
      "wnorms_dict = {wnorm: w for w, wnorm in enumerate(wnorms)}\n",
      "\n",
      "new_docs = []\n",
      "for fname, meta, content in docs:\n",
      "    new_content = np.empty((len(content),), dtype=[('word', h5py.special_dtype(vlen=unicode)),\n",
      "                                                  ('w', np.int32),\n",
      "                                                  ('start', np.int32),\n",
      "                                                  ('end', np.int32)])\n",
      "    new_content['word'] = content['word']\n",
      "    new_content['start'] = content['start']\n",
      "    new_content['end'] = content['end']\n",
      "    new_content['w'] = [wnorms_dict[wnorm] for wnorm in content['wnorm'].astype('U')]\n",
      "    \n",
      "    new_docs.append((fname, meta, new_content))\n",
      "    \n",
      "docs = new_docs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nwd = np.zeros((len(dictionary), len(docs)), dtype=np.int32)\n",
      "for d, (_, _, content) in enumerate(docs):\n",
      "    for w in content['w']:\n",
      "        nwd[w, d] += 1\n",
      "\n",
      "nwd_sp = scipy.sparse.coo_matrix(nwd)\n",
      "nwd_sp_arr = np.array(map(tuple, np.vstack([nwd_sp.data, nwd_sp.col, nwd_sp.row]).T),\n",
      "                      dtype=[('data', np.int32),\n",
      "                             ('col', np.int32),\n",
      "                             ('row', np.int32)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "metadata = []\n",
      "for fname, meta, cont in new_docs:\n",
      "    fname = fname.replace('web/static/docsdata/', '').replace('.html', '')\n",
      "\n",
      "    year, conference = fname.split('/')[-2].split('-')\n",
      "    conference, confnum = re.match(r'^(\\D+)(\\d+)$', conference).groups()\n",
      "    year = int(year)\n",
      "    confnum = int(confnum)\n",
      "    \n",
      "    author_fw = re.search(r'\\w{3,}', meta['author'], re.U).group(0)\n",
      "    title_fw = re.search(r'\\w{3,}', meta['title'], re.U).group(0)\n",
      "    slug = author_fw + str(year)[2:] + title_fw\n",
      "    slug = slug.lower()\n",
      "\n",
      "    metadata.append((fname, meta['title'], meta['author'], slug, conference, confnum, year))\n",
      "    \n",
      "metadata = np.array(metadata, dtype=[('filename', h5py.special_dtype(vlen=unicode)),\n",
      "                                     ('title', h5py.special_dtype(vlen=unicode)),\n",
      "                                     ('author', h5py.special_dtype(vlen=unicode)),\n",
      "                                     ('slug', h5py.special_dtype(vlen=unicode)),\n",
      "                                     ('conference', h5py.special_dtype(vlen=unicode)),\n",
      "                                     ('confnum', np.int32),\n",
      "                                     ('year', np.int32)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "t_init = 50\n",
      "\n",
      "em = py_artm.plsa.PlsaEmRational(nwd_sp.astype(np.float32),\n",
      "                                 t_init,\n",
      "                                 [py_artm.quantities.Perplexity(), py_artm.quantities.TopicsLeft(),\n",
      "                                  py_artm.stop_conditions.ValueBounds('topics_left', lo=2)])\n",
      "em.iterate(2000, quiet=2)\n",
      "pwt = em.phi\n",
      "ptd = em.theta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "docs_contents = []\n",
      "\n",
      "for d, (_, _, content) in enumerate(docs):\n",
      "    pwts = pwt * ptd[:, d].T\n",
      "    pwts /= pwts.sum(1)[:,np.newaxis]\n",
      "    pwts[np.isnan(pwts.sum(1)), :] = 0\n",
      "\n",
      "    topics_glob = ptd[:,d].argsort()[::-1][:np.count_nonzero(ptd[:,d])]\n",
      "    doc_content = []\n",
      "\n",
      "    for word, w, start, end in content:\n",
      "        pts = pwts[w]\n",
      "        topics = pts.argsort()[::-1][:5]\n",
      "        pts_glob = pts[topics_glob]\n",
      "        pts = pts[topics]\n",
      "\n",
      "        doc_content.append((w, start, end, topics, pts, pts_glob))\n",
      "\n",
      "    docs_contents.append((topics_glob,\n",
      "                         np.array(doc_content,\n",
      "                                 dtype=[('w', np.int32),\n",
      "                                        ('startpos', np.int32),\n",
      "                                        ('endpos', np.int32),\n",
      "                                        ('topics', np.int32, 5),\n",
      "                                        ('pts', np.float32, 5),\n",
      "                                        ('pts_glob', np.float32, len(topics_glob))])))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with h5py.File('data_.hdf', mode='w') as f:\n",
      "    f.create_dataset('p_wt', data=pwt, compression='lzf')\n",
      "    f.create_dataset('p_td', data=ptd, compression='lzf')\n",
      "    f.create_dataset('n_wd_coo', data=nwd_sp_arr, compression='lzf')\n",
      "    f.create_dataset('dictionary', data=dictionary, compression='lzf')\n",
      "    f.create_dataset('metadata', data=metadata, compression='lzf')\n",
      "    \n",
      "    docs_group = f.create_group('documents')\n",
      "    for i, (topics, doc) in enumerate(docs_contents):\n",
      "        dset = docs_group.create_dataset('%d' % i, data=doc, compression='lzf')\n",
      "        dset.attrs['topics'] = topics"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!mv data_.hdf data.hdf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from whoosh import fields, analysis, index, qparser, scoring, query, spelling, automata, highlight, sorting, formats\n",
      "from pickle import dumps, loads\n",
      "import h5py, codecs, re, numpy as np, scipy.sparse\n",
      "from collections import defaultdict\n",
      "from itertools import groupby"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "sys.path.append('web')\n",
      "from search import WithFloatData, RemoveDuplicatesFilter, vector_data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$p(d | t) = \\frac{p(d, t)}{p(t)} = \\frac{p(t | d) p(d)}{p(t)}$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class RegexTokenizerFilter(analysis.Filter):\n",
      "    \n",
      "    def __init__(self, expression=r'\\S+'):\n",
      "        self.expression = re.compile(expression)\n",
      "    \n",
      "    def __call__(self, tokens):\n",
      "        for t in tokens:\n",
      "            for m in self.expression.finditer(t.text):\n",
      "                t.text = m.group(0)\n",
      "                yield t"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class WithFilterFloatData(formats.PositionBoosts):\n",
      "\n",
      "    def word_values(self, value, analyzer, **kwargs):\n",
      "        fb = self.field_boost\n",
      "        seen = defaultdict(list)\n",
      "\n",
      "        for text, val in value:\n",
      "            d = np.random.randint(0, 100)\n",
      "            t = analysis.Token(positions=True, text=tit, pos=d, boost=val)\n",
      "            for tt in analyzer([t]):\n",
      "                seen[tt.text].append((tt.pos, tt.boost))\n",
      "                \n",
      "        for text, poses in seen.iteritems():\n",
      "            yield (text, len(poses), fb * sum(p[1] for p in poses), self.encode(poses))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# %%timeit\n",
      "fil = (RegexTokenizerFilter('\\S+') |\n",
      "       analysis.NgramFilter(3, 4))\n",
      "\n",
      "for i, (tit, p) in enumerate(doc_titles):\n",
      "    d = np.random.randint(0, 100)\n",
      "    t = analysis.Token(positions=True, text=tit, pos=d, boost=p)\n",
      "    list(fil([t]))\n",
      "#     print t\n",
      "#     for tt in fil([t]):\n",
      "#         print tt\n",
      "#     break"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "analyzer = (analysis.RegexTokenizer(r'\\S+') |\n",
      "            analysis.MultiFilter(index=analysis.IntraWordFilter(mergewords=True, mergenums=True), query=analysis.IntraWordFilter()) |\n",
      "            analysis.SubstitutionFilter(r'^\\W+', '') |\n",
      "            analysis.SubstitutionFilter(r'\\W+$', '') |\n",
      "            analysis.StopFilter(lang='ru') |\n",
      "            analysis.TeeFilter(analysis.PassFilter(),\n",
      "                               analysis.LowercaseFilter(),\n",
      "                               analysis.StemFilter(lang='ru')) |\n",
      "            analysis.StopFilter(stoplist=set()) |\n",
      "            RemoveDuplicatesFilter())\n",
      "\n",
      "topics_field = fields.FieldType(WithFloatData(), \n",
      "                                analyzer=analysis.IDAnalyzer(), \n",
      "                                stored=False,\n",
      "                                scorable=True,\n",
      "                                vector=WithFloatData())\n",
      "\n",
      "docs_schema = fields.Schema(d=fields.NUMERIC(stored=True, unique=True),\n",
      "                           fname=fields.ID(stored=True, unique=True),\n",
      "                           slug=fields.ID(stored=True, unique=True),\n",
      "                           topics=topics_field,\n",
      "                           content=fields.TEXT(stored=False, spelling=True, phrase=True, chars=True, analyzer=analyzer),\n",
      "                           title=fields.TEXT(stored=True, spelling=True, analyzer=analyzer, field_boost=3.0),\n",
      "                           authors=fields.TEXT(stored=True, spelling=True, field_boost=2.0),\n",
      "                           conference=fields.ID(stored=True),\n",
      "                           year=fields.NUMERIC(stored=True),\n",
      "\n",
      "                           title_ngrams=fields.NGRAMWORDS(stored=True, field_boost=2.0),\n",
      "                           authors_ngrams=fields.NGRAMWORDS(stored=True, field_boost=2.0),\n",
      "                           authors_tags=fields.KEYWORD(stored=True, commas=True))\n",
      "\n",
      "# docswords_field = fields.FieldType(WithFilterFloatData(),\n",
      "#                                    analyzer=fil,\n",
      "#                                    stored=False,\n",
      "#                                    scorable=True,\n",
      "#                                    vector=WithFilterFloatData())\n",
      "\n",
      "topics_schema = fields.Schema(t=fields.NUMERIC(stored=True, unique=True),\n",
      "                              p=fields.NUMERIC(stored=True, numtype=float),\n",
      "                              docslugs=topics_field,\n",
      "                              doctitles=topics_field,\n",
      "                              docauthors=topics_field,\n",
      "                              words=topics_field)\n",
      "\n",
      "words_schema = fields.Schema(w=fields.NUMERIC(stored=True, unique=True),\n",
      "                             n=fields.NUMERIC(stored=True),\n",
      "                             word=fields.TEXT(stored=True, spelling=True, analyzer=analyzer),\n",
      "#                              topics=topics_field,\n",
      "                             \n",
      "                             word_ngrams=fields.NGRAM(stored=True))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 107
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!rm -rf whoosh_ix\n",
      "!mkdir whoosh_ix"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!ls whoosh_ix -lh\n",
      "# docs_ix = index.create_in('whoosh_ix', docs_schema, 'docs')\n",
      "# topics_ix = index.create_in('whoosh_ix', topics_schema, 'topics')\n",
      "words_ix = index.create_in('whoosh_ix', words_schema, 'words')\n",
      "!ls whoosh_ix -lh"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "total 87M\r\n",
        "-rw-r--r-- 1 alexander alexander  51M Sep 28 09:24 docs_0nlyhvdm6jiedy03.seg\r\n",
        "-rw-r--r-- 1 alexander alexander  31K Sep 28 09:24 _docs_1.toc\r\n",
        "-rwxr-xr-x 1 alexander alexander    0 Sep 28 09:22 docs_WRITELOCK\r\n",
        "-rw-r--r-- 1 alexander alexander 1.3K Sep 29 06:37 _topics_1.toc\r\n",
        "-rw-r--r-- 1 alexander alexander  13M Sep 29 06:37 topics_ozc3wre8b8lup2vj.seg\r\n",
        "-rwxr-xr-x 1 alexander alexander    0 Sep 28 10:17 topics_WRITELOCK\r\n",
        "-rw-r--r-- 1 alexander alexander  30K Sep 29 09:47 _words_1.toc\r\n",
        "-rw-r--r-- 1 alexander alexander  24M Sep 29 09:47 words_iiwxa4s5gn0yn4se.seg\r\n",
        "-rwxr-xr-x 1 alexander alexander    0 Sep 29 08:41 words_WRITELOCK\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "total 87M\r\n",
        "-rw-r--r-- 1 alexander alexander  51M Sep 28 09:24 docs_0nlyhvdm6jiedy03.seg\r\n",
        "-rw-r--r-- 1 alexander alexander  31K Sep 28 09:24 _docs_1.toc\r\n",
        "-rwxr-xr-x 1 alexander alexander    0 Sep 28 09:22 docs_WRITELOCK\r\n",
        "-rw-r--r-- 1 alexander alexander 1.3K Sep 29 06:37 _topics_1.toc\r\n",
        "-rw-r--r-- 1 alexander alexander  13M Sep 29 06:37 topics_ozc3wre8b8lup2vj.seg\r\n",
        "-rwxr-xr-x 1 alexander alexander    0 Sep 28 10:17 topics_WRITELOCK\r\n",
        "-rw-r--r-- 1 alexander alexander  29K Sep 29 09:49 _words_0.toc\r\n",
        "-rw-r--r-- 1 alexander alexander  24M Sep 29 09:47 words_iiwxa4s5gn0yn4se.seg\r\n",
        "-rwxr-xr-x 1 alexander alexander    0 Sep 29 08:41 words_WRITELOCK\r\n"
       ]
      }
     ],
     "prompt_number": 105
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "with docs_ix.writer() as writer, h5py.File('data.hdf', mode='r') as h5f:\n",
      "    ptd = h5f['p_td'][...]\n",
      "    \n",
      "    for d, (fname, title, author, slug, conf, _, year) in enumerate(h5f['metadata'][...]):\n",
      "        with codecs.open('web/static/docsdata/%s.html' % fname, encoding='utf-8') as f:\n",
      "            html = f.read()\n",
      "            m = re.search(r'</header>(.*)</body>', html, re.DOTALL)\n",
      "            if m is None:\n",
      "                print fname\n",
      "                continue\n",
      "            html = m.group(1)\n",
      "            content = re.sub('<[^<]+?>', ' ', html)\n",
      "            \n",
      "        topics = ptd[:,d].argsort()[::-1]\n",
      "        topics = [(unicode(t), p)\n",
      "                  for t, p in zip(topics, ptd[topics,d])\n",
      "                  if p > 0]\n",
      "        topics.sort(key=lambda t: t[1], reverse=True)\n",
      "            \n",
      "        data = {\n",
      "            'd': d,\n",
      "            'fname': unicode(fname),\n",
      "            'slug': unicode(slug),\n",
      "            'content': unicode(content),\n",
      "            'title': unicode(title),\n",
      "            'authors': unicode(author),\n",
      "            'conference': unicode(conf),\n",
      "            'year': year,\n",
      "            'topics': topics,\n",
      "        }\n",
      "        docdata = {name: data[name.split('_')[0]]\n",
      "                   for name in docs_schema.names()}\n",
      "        writer.add_document(**docdata)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2011-MMRO15/Lepskiy\n",
        "CPU times: user 2min 9s, sys: 1.2 s, total: 2min 10s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 2min 19s\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "with topics_ix.writer() as writer, h5py.File('data.hdf', mode='r') as h5f:\n",
      "    ptd = h5f['p_td'][...]\n",
      "    pwt = h5f['p_wt'][...]\n",
      "    nwd = h5f['n_wd_coo'][...]\n",
      "    nwd = scipy.sparse.coo_matrix((nwd['data'], (nwd['row'], nwd['col'])))\n",
      "    nd = nwd.sum(0).A1\n",
      "    pt = ptd.dot(1.0 * nd / nd.sum())\n",
      "    \n",
      "    words_all = h5f['dictionary'][...]\n",
      "    docs_all = h5f['metadata'][...]\n",
      "    \n",
      "    for t, p in enumerate(pt):\n",
      "        pw = pwt[:, t]\n",
      "        pd = ptd[t, :]\n",
      "        \n",
      "        ws = pw.argsort()[::-1]\n",
      "        ds = pd.argsort()[::-1]\n",
      "        \n",
      "        words = words_all[ws]\n",
      "        docs = docs_all[ds]\n",
      "        \n",
      "        words = [(word, wp)\n",
      "                 for word, wp in zip(words, pw[ws])\n",
      "                 if wp > 0]\n",
      "        words.sort(key=lambda w: w[1], reverse=True)\n",
      "        \n",
      "        doc_slugs = filter(lambda d: d[1] > 0, zip(docs['slug'], pd[ds]))\n",
      "        doc_slugs.sort(key=lambda d: d[1], reverse=True)\n",
      "        \n",
      "        doc_titles = filter(lambda d: d[1] > 0, zip(docs['title'], pd[ds]))\n",
      "        doc_titles.sort(key=lambda d: d[1], reverse=True)\n",
      "        \n",
      "        doc_authors = [(author.strip(), dp / len(authors.split(',')))\n",
      "                       for authors, dp in zip(docs['author'], pd[ds]) \n",
      "                       for author in authors.split(',')]\n",
      "        doc_authors.sort(key=lambda d: d[0])\n",
      "        doc_authors = [(key, sum(d[1] for d in gr)) \n",
      "                       for key, gr in groupby(doc_authors, lambda d: d[0])]\n",
      "        doc_authors = filter(lambda d: d[1] > 0, doc_authors)\n",
      "        doc_authors.sort(key=lambda d: d[1], reverse=True)\n",
      "            \n",
      "        data = {\n",
      "            't': t,\n",
      "            'p': p,\n",
      "            'words': words,\n",
      "            'docslugs': doc_slugs,\n",
      "            'doctitles': doc_titles,\n",
      "            'docauthors': doc_authors,\n",
      "        }\n",
      "        docdata = {name: data[name.split('_')[0]]\n",
      "                   for name in topics_schema.names()}\n",
      "        writer.add_document(**docdata)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 14.5 s, sys: 160 ms, total: 14.7 s\n",
        "Wall time: 15.2 s\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "with words_ix.writer() as writer, h5py.File('data.hdf', mode='r') as h5f:\n",
      "    pwt = h5f['p_wt'][...]\n",
      "    nwd = h5f['n_wd_coo'][...]\n",
      "    nwd = scipy.sparse.coo_matrix((nwd['data'], (nwd['row'], nwd['col'])))\n",
      "    nw = nwd.sum(1).A1\n",
      "    \n",
      "    words_all = h5f['dictionary'][...]\n",
      "    \n",
      "    for w, (word, n, pt) in enumerate(zip(words_all, nw, pwt)):\n",
      "        ts = pt.argsort()[::-1]\n",
      "        pt = pt[ts]\n",
      "#         topics = filter(lambda t: t[1] > 0, zip(map(unicode, ts), pt))\n",
      "            \n",
      "        data = {\n",
      "            'w': w,\n",
      "            'n': n,\n",
      "            'word': word,\n",
      "#             'topics': topics,\n",
      "        }\n",
      "        docdata = {name: data[name.split('_')[0]]\n",
      "                   for name in words_schema.names()}\n",
      "        writer.add_document(**docdata)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 37.6 s, sys: 692 ms, total: 38.3 s\n",
        "Wall time: 38.3 s\n"
       ]
      }
     ],
     "prompt_number": 106
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "docs_ix = index.open_dir('whoosh_ix', readonly=True, indexname='docs')\n",
      "topics_ix = index.open_dir('whoosh_ix', readonly=True, indexname='topics')\n",
      "words_ix = index.open_dir('whoosh_ix', readonly=True, indexname='words')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "qp = qparser.MultifieldParser(['word', 'word_ngrams'],\n",
      "                              schema=words_ix.schema,\n",
      "                              termclass=query.Term)\n",
      "\n",
      "topics = np.random.randint(0, 50, 30)\n",
      "\n",
      "with words_ix.searcher() as searcher, h5py.File('data.hdf', mode='r') as h5f:\n",
      "    pwt = h5f['p_wt'][...]\n",
      "    \n",
      "#     q = query.Every()\n",
      "    q = qp.parse(u'\u0440\u0435\u0433\u0443\u043b\u044f\u0440')\n",
      "    print unicode(q)\n",
      "    results = searcher.search(q, terms=True, limit=None)\n",
      "    print len(results)\n",
      "    print\n",
      "    results.highlighter.fragmenter = highlight.WholeFragmenter()\n",
      "    \n",
      "#     print list(results.groups().dict)\n",
      "\n",
      "    ws_matched = np.array(sorted(hit.docnum for hit in results))\n",
      "    highlights = {hit.docnum: hit.highlights('word') for hit in results}\n",
      "    ptw_matched = pwt[ws_matched].T\n",
      "    \n",
      "    ts = ptw_matched.sum(1).argsort()[::-1]\n",
      "    \n",
      "    for t, pws in zip(ts, ptw_matched[ts]):\n",
      "        if sum(pws) == 0:\n",
      "            break\n",
      "        print t, sum(pws)\n",
      "        ws_cur = ws_matched[pws.nonzero()[0]]\n",
      "        print ws_cur\n",
      "\n",
      "#     for i, hit in enumerate(results):\n",
      "#         print hit['word'], hit.score\n",
      "#         print hit.highlights('word') or hit.highlights('word_ngrams')\n",
      "#         for f, t in hit.matched_terms():\n",
      "#             print f, t\n",
      "#         print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(word:\u0440\u0435\u0433\u0443\u043b\u044f\u0440 OR (word_ngrams:\u0440\u0435\u0433\u0443 AND word_ngrams:\u0435\u0433\u0443\u043b AND word_ngrams:\u0433\u0443\u043b\u044f AND word_ngrams:\u0443\u043b\u044f\u0440))\n",
        "18\n",
        "\n",
        "16 0.00798695905542\n",
        "[ 8224 12733 12734]\n",
        "1 0.00425618812005\n",
        "[12724 12729]\n",
        "22 0.00373606981884\n",
        "[ 5212 12726 12732 12733 12734]\n",
        "48 0.00318539270302\n",
        "[ 8223 12724 12725 12726 12727]\n",
        "28 0.00248969891982\n",
        "[12733 12734]\n",
        "49 0.00245562215423\n",
        "[ 8223 12724 12728 12732 12733 12734]\n",
        "32 0.00241732238283\n",
        "[12722 12726 12733 12734]\n",
        "20 0.00216602514774\n",
        "[10026 12722 12723 12724 12726 12728 12732 12734]\n",
        "12 0.00162842635473\n",
        "[12724 12728 12729 12731]\n",
        "35 0.00122916398686\n",
        "[12722 12724 12729 12730 12734]\n",
        "7 0.00108655932127\n",
        "[ 8224 12734]\n",
        "47 0.000900968094356\n",
        "[12734]\n",
        "25 0.00075993250357\n",
        "[ 8223 12734]\n",
        "17 0.000695244430972\n",
        "[12733 12734]\n",
        "11 0.000447370868642\n",
        "[12734]\n",
        "24 0.000327137786371\n",
        "[5211 5212 8224]\n",
        "19 0.000262031317106\n",
        "[12732 12734]\n",
        "33 0.000231687932683\n",
        "[12724 12729 12734]\n",
        "0 0.000226470045163\n",
        "[ 8224 12732 12734]\n",
        "23 0.000208484212635\n",
        "[12723]\n",
        "38 0.00020485213463\n",
        "[ 8224 12734]\n",
        "46 0.000190572885913\n",
        "[8224]\n",
        "9 0.000179017413757\n",
        "[12734]\n",
        "42 0.000177330162842\n",
        "[12732]\n",
        "14 0.00017648593348\n",
        "[12729]\n",
        "3 0.000172804640897\n",
        "[12732 12734]\n",
        "45 0.000163882767083\n",
        "[12734]\n",
        "10 0.000142945882544\n",
        "[12722 12724 12733 12734]\n",
        "43 0.000130621690914\n",
        "[12729 12731]\n",
        "27 0.000122417419334\n",
        "[12724]\n",
        "18 0.000106129933556\n",
        "[12734]\n",
        "30 7.55267683417e-05\n",
        "[12734]\n",
        "8 5.16453692398e-05\n",
        "[8224]\n",
        "CPU times: user 52 ms, sys: 0 ns, total: 52 ms\n",
        "Wall time: 48.6 ms\n"
       ]
      }
     ],
     "prompt_number": 216
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "raise"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "exceptions must be old-style classes or derived from BaseException, not NoneType",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-189-26814ed17a01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mTypeError\u001b[0m: exceptions must be old-style classes or derived from BaseException, not NoneType"
       ]
      }
     ],
     "prompt_number": 189
    }
   ],
   "metadata": {}
  }
 ]
}
{
 "metadata": {
  "name": "",
  "signature": "sha256:37df89af81932afc096395df7fd1644bd1792736ed588cfd3e500a322b14b8a1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import codecs\n",
      "import numpy as np\n",
      "import pandas\n",
      "import scipy.sparse\n",
      "from scipy.ndimage.filters import convolve1d\n",
      "import sys\n",
      "import comp_exp\n",
      "import py_artm\n",
      "import h5py\n",
      "sys.path.append('/home/alexander/strijov/MLAlgorithms/Group174/Plavin2014TopicsNumberOptimization/code')\n",
      "from utils import *\n",
      "from ggplot_utils import *\n",
      "from IPython.display import display\n",
      "from functools import partial\n",
      "from itertools import count, groupby\n",
      "from ipy_progressbar import ProgressBar\n",
      "from itertools import starmap\n",
      "import codecs\n",
      "from pprint import pprint\n",
      "from recordtype import recordtype\n",
      "import re\n",
      "from glob import glob\n",
      "\n",
      "from rpy2.robjects import pandas2ri\n",
      "pandas2ri.activate()\n",
      "import rpy2.robjects as robj\n",
      "from rpy2.robjects.lib import ggplot2 as gg\n",
      "from rpy2.ipython.ggplot import ggplot as iggplot\n",
      "from rpy2.robjects.packages import importr\n",
      "rbase = importr('base')\n",
      "\n",
      "%matplotlib inline\n",
      "%load_ext rpy2.ipython.rmagic\n",
      "\n",
      "from ggplot_utils import *\n",
      "MyGGPlot.latex_enabled = False"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/lib/python2.7/distutils/dist.py:267: UserWarning: Unknown distribution option: 'library_dirs'\n",
        "  warnings.warn(msg)\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ds = []\n",
      "ws = []\n",
      "cnts = []\n",
      "\n",
      "with open('MMRO_new.txt') as nwdf:\n",
      "    D = int(nwdf.readline())\n",
      "    W = int(nwdf.readline())\n",
      "    \n",
      "    for line in nwdf:\n",
      "        d, w, cnt = map(int, line.split())\n",
      "        ds.append(d)\n",
      "        ws.append(w)\n",
      "        cnts.append(cnt)\n",
      "        \n",
      "nwd = scipy.sparse.coo_matrix((cnts, (ws, ds))).astype(np.float32)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nwd.shape, sum(nwd.sum(1).A1 == 0), sum(nwd.sum(0).A1 == 0), 13031-668"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "((12363, 1009), 0, 0, 12363)"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('MMRO_new.txt', 'w') as nwdf:\n",
      "    new_nwd = nwd.tocsr()[np.nonzero(nwd.sum(1).A1)[0], :].tocoo()\n",
      "    \n",
      "    nwdf.write(str(new_nwd.shape[1]) + '\\n')\n",
      "    nwdf.write(str(new_nwd.shape[0]) + '\\n')\n",
      "    \n",
      "    for w, d, cnt in zip(new_nwd.row, new_nwd.col, new_nwd.data):\n",
      "        nwdf.write('%d %d %d' % (d, w, cnt) + '\\n')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tau = 0.2\n",
      "t_init = 50\n",
      "\n",
      "em = py_artm.plsa.PlsaEmRational(nwd,\n",
      "                                 t_init,\n",
      "                                 [py_artm.quantities.Perplexity(), py_artm.quantities.TopicsLeft(),\n",
      "                                  py_artm.stop_conditions.ValueBounds('topics_left', lo=2)])\n",
      "em.iterate(1000, quiet=1)\n",
      "pwt = em.phi\n",
      "ptd = em.theta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with codecs.open('authors.txt', encoding='utf-8') as authf:\n",
      "    lines = [line.strip() for line in authf]\n",
      "    docinfos = []\n",
      "    i = 0\n",
      "    while i < len(lines):\n",
      "        assert int(lines[i]) * 3 == i\n",
      "        assert int(lines[i]) == len(docinfos)\n",
      "        i += 1\n",
      "        docinfo = []\n",
      "        while i < len(lines) and not lines[i].isdigit():\n",
      "            docinfo.append(lines[i])\n",
      "            i += 1\n",
      "        assert len(docinfo) == 2\n",
      "        docinfos.append(tuple(docinfo))\n",
      "    docinfos = np.array(docinfos)\n",
      "    docinfos = np.array(docinfos, dtype=h5py.special_dtype(vlen=unicode))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with codecs.open('dictionary_new.txt', encoding='utf-8') as dictf:\n",
      "    assert nwd.shape[0] == int(dictf.readline().strip())\n",
      "    indwords = [line.strip().split(' ', 1) for line in dictf]\n",
      "    indwords[723] = ['723', '']\n",
      "    assert all(int(ind) == i for i, (ind, w) in enumerate(indwords))\n",
      "    words = [w.lower() for ind, w in indwords]\n",
      "    words = np.array(words, dtype=h5py.special_dtype(vlen=unicode))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with codecs.open('dictionary_new.txt', mode='w', encoding='utf-8') as dictf:\n",
      "    words_new = words[np.nonzero(nwd.sum(1).A1)[0]]\n",
      "    dictf.write(str(words_new.shape[0]) + '\\n')\n",
      "    for i, word in enumerate(words_new):\n",
      "        dictf.write('%d %s\\n' % (i, word.upper()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 94
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "words_dict = {word: w for w, word in enumerate(words)}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with codecs.open('MMROwithOrder.txt', encoding='utf-8') as docsf:\n",
      "    lines = [line.strip() for line in docsf]\n",
      "    docs = []\n",
      "    i = 0\n",
      "    while i < len(lines):\n",
      "        assert int(lines[i]) == len(docs)\n",
      "        i += 1\n",
      "        d = len(docs)\n",
      "        \n",
      "        pwts = pwt * ptd[:, d].T\n",
      "        pwts /= pwts.sum(1)[:,np.newaxis]\n",
      "        pwts[np.isnan(pwts.sum(1)), :] = 0\n",
      "        \n",
      "        topics_glob = ptd[:,d].argsort()[::-1][:np.count_nonzero(ptd[:,d])]\n",
      "        \n",
      "        doc = []\n",
      "        while i < len(lines) and not lines[i].isdigit():\n",
      "            try:\n",
      "                word, word_norm = lines[i].lower().split(' ', 1)\n",
      "            except ValueError:\n",
      "                word = lines[i].lower().strip()\n",
      "                word_norm = word\n",
      "\n",
      "            # dictionary lookup\n",
      "            try:\n",
      "                w = words_dict[word_norm]\n",
      "            except KeyError:\n",
      "                w = -1\n",
      "                \n",
      "            # topics info\n",
      "            pts = pwts[w]\n",
      "            topics = pts.argsort()[::-1][:5]\n",
      "            pts_glob = pts[topics_glob]\n",
      "            pts = pts[topics]\n",
      "                \n",
      "            doc.append((word, w, topics, pts, pts_glob))\n",
      "            i += 1\n",
      "\n",
      "        docs.append((topics_glob,\n",
      "                     np.array(map(tuple, doc),\n",
      "                             dtype=[('word', h5py.special_dtype(vlen=unicode)),\n",
      "                                    ('w', np.int32),\n",
      "                                    ('topics', np.int32, 5),\n",
      "                                    ('pts', np.float32, 5),\n",
      "                                    ('pts_glob', np.float32, len(topics_glob))])))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('filenames_new.txt') as fnamesf:\n",
      "    docfnames = [line.strip().split(' ', 1) for line in fnamesf]\n",
      "    docfnames = [(int(d), fname) for d, fname in docfnames]\n",
      "    filenames = np.zeros((max(d for d, fname in docfnames) + 1,), dtype=h5py.special_dtype(vlen=str))\n",
      "    for d, fname in docfnames:\n",
      "        filenames[d] = fname"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with h5py.File('data.hdf', mode='w') as f:\n",
      "    f.create_dataset('p_wt', data=pwt, compression='lzf')\n",
      "    f.create_dataset('p_td', data=ptd, compression='lzf')\n",
      "    f.create_dataset('p_wd', data=np.dot(pwt, ptd), compression='lzf')\n",
      "    f.create_dataset('n_wd', data=nwd.A, dtype=np.int32, compression='lzf')\n",
      "    f.create_dataset('dictionary', data=words, compression='lzf')\n",
      "    f.create_dataset('docinfo', data=docinfos, compression='lzf')\n",
      "    f.create_dataset('filenames', data=filenames, compression='lzf')\n",
      "    \n",
      "    docs_group = f.create_group('documents')\n",
      "    for i, (topics, doc) in enumerate(docs):\n",
      "        dset = docs_group.create_dataset('%d' % i, data=doc, compression='lzf')\n",
      "        dset.attrs['topics'] = topics"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('filenames.txt') as fnamesf:\n",
      "    docfnames = [line.strip().split(' ', 1) for line in fnamesf]\n",
      "    docfnames = [(int(d), fname) for d, fname in docfnames]\n",
      "    d_to_fname = {d: fname for d, fname in docfnames}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('filenames.txt') as fnamesf:\n",
      "    docfnames = [line.strip().split(' ', 1)[1] for line in fnamesf]\n",
      "    docfiles = [(fname, open('documents/' + fname).read()) for fname in docfnames]\n",
      "\n",
      "full_xml_files = [(fname, open(fname).read()) for fname in glob('documents/xml/*/*')]\n",
      "fname_to_xml = {}\n",
      "\n",
      "for fname, fcont in docfiles:\n",
      "    matches = [fn for fn, fc in full_xml_files if fc == fcont]\n",
      "    if len(matches) == 1:\n",
      "        fname_to_xml[fname] = matches[0].replace('documents/xml/XML_', '').replace('.xml', '')\n",
      "    else:\n",
      "        print fname, matches"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xml_to_tex = {}\n",
      "\n",
      "tex_fnames = glob('documents/tex/*/*')\n",
      "for xml_fname in glob('documents/xml/*/*'):\n",
      "    fname = xml_fname.replace('/xml/', '/tex/').replace('/XML_', '/').replace('.xml', '.tex').replace('unequated_', '')\n",
      "    matches = [tex_fname for tex_fname in tex_fnames if fname.lower() == tex_fname.lower()]\n",
      "    if len(matches) == 1:\n",
      "        xml_to_tex[xml_fname.replace('documents/xml/XML_', '').replace('.xml', '')] = matches[0].replace('documents/tex/', '')\n",
      "    else:\n",
      "        print xml_fname, matches"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "documents/xml/XML_2007-MMRO13/unequated_DeWansa_SI_1 - copy.xml []\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d_to_tex = {}\n",
      "\n",
      "for d in range(1009):\n",
      "    fname = d_to_fname.get(d, None)\n",
      "    full = fname_to_xml.get(fname, None)\n",
      "    tex = xml_to_tex.get(full, None)\n",
      "    if tex is None:\n",
      "        print d, fname, full, tex\n",
      "    else:\n",
      "        d_to_tex[d] = tex"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "391 unequated_DeWansa_SI_1 - copy.xml 2007-MMRO13/unequated_DeWansa_SI_1 - copy None\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d_to_html = {}\n",
      "\n",
      "for d, tex in d_to_tex.items():\n",
      "    d_to_html[d] = re.sub('.tex$', '', tex, flags=re.I)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('filenames_new.txt', 'w') as f:\n",
      "    for d, tex in sorted(d_to_html.items()):\n",
      "        f.write('%d %s\\n' % (d, tex))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for fname in ProgressBar(glob('documents/tex/*/*')):\n",
      "    outfname = fname.replace('/tex/', '/html/') + '.htex'\n",
      "    outdir = outfname[::-1].split('/', 1)[1][::-1]\n",
      "    print fname, '->', outfname\n",
      "    !mkdir -p {outdir}\n",
      "    !rm {outfname}\n",
      "    !iconv -f cp1251 -t utf-8 {fname} | perl -pe '/^[^%]*%%/ && next; /\\\\%/ && next; s/%.*\\n//g' | pandoc -f latex -t html5 -o {outfname} --standalone --gladtex"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for fname in ProgressBar(glob('documents/html/*/*.htex')):\n",
      "    outfname = re.sub('.tex.htex$', '.html', fname, flags=re.I)\n",
      "    print fname, '->', outfname\n",
      "    !gladtex -d {outfname.replace('.html', '')} {fname}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "htexs = glob('documents/html/*/*.htex')\n",
      "htmls = glob('documents/html/*/*.html')\n",
      "emptyfnames = !find documents -type f -empty\n",
      "\n",
      "for texfname in glob('documents/tex/*/*'):\n",
      "    htexfname = texfname.replace('/tex/', '/html/') + '.htex'\n",
      "    htmlfname = re.sub('.tex.htex$', '.html', htexfname, flags=re.I)\n",
      "    ishtex = htexfname in htexs\n",
      "    ishtml = htmlfname in htmls\n",
      "    isempty = (htexfname in emptyfnames) or (htmlfname in emptyfnames)\n",
      "\n",
      "    if not ishtex or not ishtml or isempty:\n",
      "        print '-+'[ishtex], '-+'[ishtml], '+-'[isempty], texfname\n",
      "#         !iconv -f cp1251 -t utf-8 '{texfname}' | sed 's/\\r$//g' | sed 's/\\r/\\n/g' | sed -e '/.$/a\\' | perl -pe '/\\\\%/ && next; s/%.*\\n//g' | pandoc -f latex -t html5 -o '{htexfname}' --standalone --gladtex\n",
      "#         !gladtex -v -d '{htmlfname.replace('.html', '')}' '{htexfname}'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with h5py.File('data.hdf', mode='r') as h5f:    \n",
      "    docinfos = []\n",
      "    \n",
      "    for d in range(len(h5f['filenames'])):\n",
      "        filename = h5f['filenames'][d]\n",
      "        if filename != '0':\n",
      "            year, conference = filename.split('/')[0].split('-')\n",
      "        else:\n",
      "            year = 9999\n",
      "            conference = 'UNKNOWN99'\n",
      "        conference, confnum = re.match(r'^(\\D+)(\\d+)$', conference).groups()\n",
      "        year = int(year)\n",
      "        confnum = int(confnum)\n",
      "\n",
      "        authors, name = h5f['docinfo'][d]\n",
      "        if name in ['*', '**']:\n",
      "            name = 'UNKNOWN'\n",
      "\n",
      "        slug = re.search('\\w+', authors, re.U).group(0) + str(year)[2:] + re.search('\\w+', name, re.U).group(0)\n",
      "        slug = slug.lower()\n",
      "        \n",
      "        docinfos.append((conference, confnum, year, authors, name, slug))\n",
      "        \n",
      "    docinfos = np.array(docinfos, dtype=[\n",
      "        ('conference', h5py.special_dtype(vlen=str)),\n",
      "        ('confnum', np.int32),\n",
      "        ('year', np.int32),\n",
      "        ('authors', h5py.special_dtype(vlen=unicode)),\n",
      "        ('name', h5py.special_dtype(vlen=unicode)),\n",
      "        ('slug', h5py.special_dtype(vlen=unicode)),\n",
      "    ])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "too many values to unpack",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-35-345cb6f74e4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mconfnum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mauthors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'docinfo'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'*'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'**'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'UNKNOWN'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mValueError\u001b[0m: too many values to unpack"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with h5py.File('data_.hdf', mode='w') as h5f:\n",
      "    with h5py.File('data.hdf', mode='r') as h5f_s:\n",
      "        h5f.create_dataset('p_wt', data=h5f_s['p_wt'], compression='lzf')\n",
      "        h5f.create_dataset('p_td', data=h5f_s['p_td'], compression='lzf')\n",
      "        h5f.create_dataset('p_wd', data=h5f_s['p_wd'], compression='lzf')\n",
      "        h5f.create_dataset('n_wd', data=h5f_s['n_wd'], compression='lzf')\n",
      "        h5f.create_dataset('dictionary', data=h5f_s['dictionary'], compression='lzf')\n",
      "        h5f.create_dataset('docinfo', data=h5f_s['docinfo'], compression='lzf')\n",
      "        h5f.create_dataset('filenames', data=h5f_s['filenames'], compression='lzf')\n",
      "\n",
      "        docs_group = h5f.create_group('documents')\n",
      "        for i, doc in h5f_s['documents'].items():\n",
      "            dset = docs_group.create_dataset(i, data=doc, compression='lzf')\n",
      "            dset.attrs['topics'] = doc.attrs['topics']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!mv data_.hdf data.hdf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "csses = !ls web/static/docsdata/*/*.css\n",
      "for css in ProgressBar(csses):\n",
      "    parts = css.split('/')\n",
      "    repl = '/static/docsdata/%s/%s' % (parts[3], parts[4].replace('.css', '.png'))\n",
      "    !sed -i \"s|display: inline-block;||g\" '{css}'\n",
      "    !sed -i \"s|url('.*|url('{repl}');|g\" '{css}'\n",
      "    !sed -i \"s|background-image|display: inline-block; \\nbackground-image|g\" '{css}'\n",
      "#     !cat {css}\n",
      "#     break"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$p(t | d, w) = \\frac{p(t, d, w)}{p(d, w)} = \\frac{p(w, t | d) p(d)}{p(w, d)} = \\frac{p(w | t, d) p(t | d) p(d)}{p(w, d)} = \\frac{p(w | t) p(t | d) p(d)}{p(w, d)} = \\frac{p(w | t) p(t | d) p(d)}{p(w | d) p(d)} = \\frac{p(w | t) p(t | d)}{p(w | d)}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$p(t | d, w) = p(w | t) p(t | d)$$\n",
      "$$\\sum_{d,w} p(t | d, w) \\frac{n_{wd}}{n_d} p(d) = \\sum_{d,w} p(t | d, w) p(w | d) p(d) = \\sum_{d,w} p(t | d, w) p(w, d) = \\sum_{d,w} p(t | d, w) \\frac{n_{wd}}{n} = p(t) = \\sum_d p(t | d) p(d) = \\sum_d p(t | d) \\frac{n_d}{n}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$p(t | w) = \\frac{p(t, w)}{p(w)} = \\frac{p(w | t) p(t)}{p(w)} = \\frac{p(w | t) p(t)}{\\sum_d p(w|d) p(d)}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u041f\u0440\u043e\u0432\u0435\u0440\u0435\u043d\u043e:\n",
      "$$p(t | d) \\approx \\sum_w p(t | d, w) p(w | d)$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with h5py.File('data.hdf', mode='r') as h5f:\n",
      "    pwt = h5f['p_wt'][...] # p(w | t)\n",
      "    ptd = h5f['p_td'][...] # p(t | d)\n",
      "    raise\n",
      "    nwd = h5f['n_wd'][...] # n_{wd}\n",
      "    nw = nwd.sum(1) # n_w\n",
      "    nd = nwd.sum(0) # n_d\n",
      "    n = nd.sum() # n\n",
      "    pw = 1.0 * nw / n # p(w) = n_w / n\n",
      "    pd = 1.0 * nd / n # p(d) = n_d / n\n",
      "    pt = ptd.dot(pd) # p(t) = \\sum_d p(t | d) p(d)\n",
      "    pwd = 1.0 * nwd / nd # p(w | d) = n_{wd} / n_d\n",
      "#     p_wd = 1.0 * nwd / n # p(w, d) = n_{wd} / n\n",
      "#     for t in range(50):\n",
      "#         ptdw = (pwt[:, t] * ptd[t, :][:, np.newaxis]).T # p(t | d, w) = p(w | t) p(t | d)\n",
      "#         print (ptdw * pwd).sum(), pt[t]\n",
      "    for d in range(1009):\n",
      "        pwts = pwt * ptd[:, d].T\n",
      "        pwts /= pwts.sum(1)[:,np.newaxis]\n",
      "        pwts[np.isnan(pwts.sum(1)), :] = 0\n",
      "        pandas.Series(ptd[:,d] - pwts.T.dot(pwd[:, d])).plot()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "exceptions must be old-style classes or derived from BaseException, not NoneType",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-7-00ef0bfe7f83>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mpwt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'p_wt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# p(w | t)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mptd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'p_td'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# p(t | d)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mnwd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'n_wd'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# n_{wd}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mnw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnwd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# n_w\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mTypeError\u001b[0m: exceptions must be old-style classes or derived from BaseException, not NoneType"
       ]
      }
     ],
     "prompt_number": 7
    }
   ],
   "metadata": {}
  }
 ]
}